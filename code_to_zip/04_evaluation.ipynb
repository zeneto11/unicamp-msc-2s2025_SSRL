{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d169c10",
   "metadata": {},
   "source": [
    "### Project: Domain Adaptation of Portuguese SLMs via Self-Supervised Fine-Tuning with LoRA\n",
    "MO436C - Introduction to Self-Supervised Learning (SSRL)\n",
    "\n",
    "**Team Members:**\n",
    "- Alejandro Núñez Arroyo. <a href=\"mailto:a299215@dac.unicamp.br\">a299215@dac.unicamp.br</a>  \n",
    "- Daniel da Costa Nunes Resende Neto. <a href=\"mailto:d169408@dac.unicamp.br\">d169408@dac.unicamp.br</a>  \n",
    "- José Augusto de Almeida Neto. <a href=\"mailto:j299218@dac.unicamp.br\">j299218@dac.unicamp.br</a>  \n",
    "\n",
    "*Instituto de Computação (IC), Universidade Estadual de Campinas (UNICAMP)*  \n",
    "*Campinas, November 2025*\n",
    "\n",
    "---\n",
    "\n",
    "#### License\n",
    "\n",
    "This notebook and its source code are released under the **GNU General Public License v3.0 (GPLv3)**.  \n",
    "You are free to use, modify, and redistribute this work under the following terms:\n",
    "\n",
    "> **GNU General Public License v3.0**  \n",
    "> Copyright © 2025 The Authors listed above  \n",
    ">\n",
    "> This program is free software: you can redistribute it and/or modify  \n",
    "> it under the terms of the GNU General Public License as published by  \n",
    "> the Free Software Foundation, either version 3 of the License, or  \n",
    "> (at your option) any later version.  \n",
    ">\n",
    "> This program is distributed in the hope that it will be useful,  \n",
    "> but **without any warranty**; without even the implied warranty of  \n",
    "> merchantability or fitness for a particular purpose. See the  \n",
    "> GNU General Public License for more details.  \n",
    ">\n",
    "> You should have received a copy of the GNU General Public License  \n",
    "> along with this program. If not, see  \n",
    "> [https://www.gnu.org/licenses/gpl-3.0.en.html](https://www.gnu.org/licenses/gpl-3.0.en.html).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375d248",
   "metadata": {},
   "source": [
    "# Notebook 4: Evaluation\n",
    "\n",
    "This notebook focuses on the **evaluation phase** of the project, in which all fine-tuned and baseline models are systematically assessed on the **Portuguese MMLU test set**.  \n",
    "It measures model accuracy under multiple answer-derivation strategies, quantifying reasoning and knowledge retention in a zero-shot setting.\n",
    "\n",
    "---\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "1. **Setup & Imports**  \n",
    "   Load all core dependencies, configure logging, and define global settings for reproducibility.\n",
    "\n",
    "2. **Evaluation Framework**  \n",
    "   - Implement a configurable evaluation pipeline for MMLU-style multiple-choice questions.  \n",
    "   - Support both **standard models** and **LoRA-adapted checkpoints**.  \n",
    "   - Include multiple evaluation strategies: greedy decoding, free-form generation, and sequence log-probability scoring.\n",
    "\n",
    "3. **Models**  \n",
    "   - Execute and compare evaluations across all variants of the *Gemma-3-1b* family:  \n",
    "\n",
    "   1. ***Gemma-3-1b-it* (Baseline)**\n",
    "   2. ***Gemma-3-1b-pt* + LoRA SFT on MMLU**\n",
    "   3. ***Gemma-3-1b-pt* + Wiki Context + LoRA SFT on MMLU**\n",
    "\n",
    "4. **Evaluation Runs**\n",
    "   - All evaluations were conducted on the **held-out MMLU test set (1,038 questions)**,\n",
    "   following a **zero-shot** evaluation protocol.\n",
    "   - Accuracy was computed for each of the three inference strategies:\n",
    "\n",
    "   1. **Greedy Decoding (Single Token)** — direct deterministic prediction.\n",
    "   2. **Free-Form Generation (Sampling)** — natural language reasoning and response.\n",
    "   3. **Sequence Log-Probability Scoring** — log-likelihood ranking across all answer options.\n",
    "\n",
    "**Output Artifacts**  \n",
    "   - `mmlu_eval.log` — runtime logs for all evaluations.  \n",
    "   - `results/*.csv` — per-model detailed results with predicted and correct answers.  \n",
    "   - Printed accuracy summaries by decoding strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21244efd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* [Part 1: Setup & Imports](#1-setup--imports)\n",
    "* [Part 2: Evaluation Framework](#2-evaluation-framework)\n",
    "  - [2.1 Evaluation Configuration](#21-evaluation-configuration)\n",
    "  - [2.2 MMLU Evaluator Class](#22-mmlu-evaluator-class)\n",
    "* [Part 3: Model Evaluation Runs](#3-model-evaluation-runs)\n",
    "  - [3.1 Gemma-3-1b-it (Baseline)](#31-gemma-3-1b-it)\n",
    "  - [3.2 Gemma-3-1b-pt + LoRA SFT on MMLU](#32-gemma-3-1b-pt--lora-sft-on-mmlu)\n",
    "  - [3.3 Gemma-3-1b-pt + Wiki Context + LoRA SFT on MMLU](#33-gemma-3-1b-pt--wiki-context--lora-sft-on-mmlu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b82cb",
   "metadata": {},
   "source": [
    "<!-- ## 1. Setup & Imports -->\n",
    "## 1. Setup & Imports <a id=\"part_01\"></a>\n",
    "\n",
    "\n",
    "Here we load the main libraries that will be used throughout this notebook.\n",
    "\n",
    "It also sets up the logger, ensuring standardized output formatting and reproducibility across experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9379b144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from loguru import logger\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d6ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger\n",
    "logger.remove()  # Remove default handler\n",
    "logger.add(\n",
    "    sink=lambda msg: print(msg, end=\"\"),\n",
    "    colorize=True,\n",
    "    format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | \"\n",
    "           \"<level>{level: <8}</level> | \"\n",
    "           \"<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - \"\n",
    "           \"<level>{message}</level>\",\n",
    ")\n",
    "# logger.add(\"mmlu_eval_gemma-3-1b-it.log\", rotation=\"5 MB\",\n",
    "logger.add(\"mmlu_eval.log\", rotation=\"5 MB\",\n",
    "           retention=\"10 days\", level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b24ffc",
   "metadata": {},
   "source": [
    "## 2. Evaluation Framework <a id=\"part_02\"></a>\n",
    "\n",
    "This section defines the **evaluation architecture** used to benchmark language models on the MMLU dataset.\n",
    "It includes configuration structures, model loading routines, prompt formatting, and metric computation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8168b",
   "metadata": {},
   "source": [
    "### 2.1 Evaluation Configuration <a id=\"21-evaluation-configuration\"></a>\n",
    "\n",
    "The configuration class centralizes all parameters controlling evaluation behavior — including model paths,\n",
    "sampling settings, generation parameters, and output file structure.\n",
    "\n",
    "This modular design ensures repeatability and allows for quick adaptation between model variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245937ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    \"\"\"Configuration for model evaluation.\"\"\"\n",
    "\n",
    "    # Model parameters\n",
    "    model_path: str\n",
    "    base_model_path: str = None\n",
    "    lora: bool = False\n",
    "\n",
    "    # Test data parameters\n",
    "    test_data_path: str = \"https://drive.google.com/uc?export=download&id=1PzCRCQZtIGV6GO4C-oq76DeIMck-t6P-\"\n",
    "    output_dir: str = \"results\"\n",
    "\n",
    "    # Sampling configuration\n",
    "    sample_size: Optional[int] = None\n",
    "    random_seed: int = 42\n",
    "\n",
    "    # Generation parameters\n",
    "    max_new_tokens_short: int = 1   # Short generation for single token - A, B, C, or D\n",
    "    max_new_tokens_long: int = 32   # Long generation for full answer explanation\n",
    "    do_sample_short: bool = False   # Use greedy decoding for short answers\n",
    "    do_sample_long: bool = True     # Use sampling for long answers\n",
    "    temperature: float = 0.5        # Temperature for sampling\n",
    "\n",
    "    # System prompt\n",
    "    system_prompt: str = (\n",
    "        \"Você é um assistente especialista que responde questões de múltipla escolha em português do Brasil.\\n\"\n",
    "        \"Responda apenas com UMA opção correta (A, B, C ou D).\\n\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def model_name(self) -> str:\n",
    "        \"\"\"Extract model name from path.\"\"\"\n",
    "        return f\"{Path(self.model_path).parent.name}_{Path(self.model_path).name}\"\n",
    "\n",
    "    @property\n",
    "    def output_path(self) -> Path:\n",
    "        \"\"\"Generate output path for results.\"\"\"\n",
    "        output_dir = Path(self.output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return output_dir / f\"{self.model_name}_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa8b8c3",
   "metadata": {},
   "source": [
    "### 2.2 MMLU Evaluator Class <a id=\"22-mmlu-evaluator-class\"></a>\n",
    "\n",
    "The `MMLUEvaluator` class encapsulates all evaluation procedures, including:\n",
    "\n",
    "1. **Model Loading**\n",
    "\n",
    "   * Supports both regular checkpoints and LoRA-adapted models.\n",
    "   * Automatically detects device configuration and precision mode.\n",
    "\n",
    "2. **Prompt Formatting**\n",
    "\n",
    "   * Standardizes multiple-choice question input format, ensuring alignment between training and evaluation.\n",
    "\n",
    "3. **Generation Strategies**\n",
    "   Implements three complementary approaches:\n",
    "\n",
    "   * **Greedy Decoding (1 token):** measures deterministic accuracy.\n",
    "   * **Free-Form Sampling:** allows longer, natural completions.\n",
    "   * **Sequence Log-Probability Scoring:** evaluates likelihood of full answer sequences.\n",
    "\n",
    "4. **Metrics and Logging**\n",
    "\n",
    "   * Computes accuracy and distribution statistics per strategy.\n",
    "   * Saves detailed predictions to CSV and logs summaries for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e1018",
   "metadata": {},
   "source": [
    "> This framework directly implements the methodology described in the report, evaluating each model\n",
    "> using three independent answer extraction methods to ensure a comprehensive performance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183adbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMLUEvaluator:\n",
    "    \"\"\"\n",
    "    MMLU-style multiple choice question evaluator for language models.\n",
    "\n",
    "    Handles model loading, inference, answer extraction, and metric calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with configuration.\n",
    "\n",
    "        Args:\n",
    "            config: EvaluationConfig instance with model and test parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        set_seed(self.config.random_seed)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.results_df = None\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Model loading\n",
    "    # -----------------------------------------------------------------\n",
    "    def load_model(self) -> None:\n",
    "        \"\"\"Load the model and tokenizer onto the device.\"\"\"\n",
    "        logger.info(f\"Device: {self.device}\")\n",
    "        logger.info(f\"Loading model: {self.config.model_path}\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.model_path,\n",
    "            dtype=torch.bfloat16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "        ).eval()\n",
    "\n",
    "        logger.info(f\"Model loaded successfully\")\n",
    "\n",
    "    def load_model_lora(self) -> None:\n",
    "        \"\"\"Load the base model and apply LoRA adapter.\"\"\"\n",
    "        logger.info(f\"Device: {self.device}\")\n",
    "        logger.info(f\"Loading BASE MODEL + LORA ADAPTER\")\n",
    "\n",
    "        # 1. Load base model\n",
    "        logger.info(f\"Base model: {self.config.base_model_path}\")\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.base_model_path,\n",
    "            dtype=torch.bfloat16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        # 2. Load LoRA adapter\n",
    "        logger.info(f\"Loading LoRA adapter: {self.config.model_path}\")\n",
    "\n",
    "        # Load tokenizer from LoRA model path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.model_path)\n",
    "\n",
    "        # Apply LoRA adapter\n",
    "        self.model = PeftModel.from_pretrained(\n",
    "            self.model,\n",
    "            self.config.model_path,\n",
    "            dtype=torch.bfloat16 if self.device == \"cuda\" else torch.float32,\n",
    "            base_model_name_or_path=self.config.base_model_path\n",
    "        )\n",
    "\n",
    "        self.model = self.model.eval()\n",
    "        logger.info(f\"Model + LoRA loaded successfully on {self.device}\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Prompt formatting\n",
    "    # -----------------------------------------------------------------\n",
    "    def format_question(self, row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Format a question row into a prompt.\n",
    "\n",
    "        Args:\n",
    "            row: DataFrame row containing question and options\n",
    "\n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        subject = row['Subject'].replace('_', ' ').title()\n",
    "        user_prompt = (\n",
    "            f\"{self.config.system_prompt}\"\n",
    "            f\"Assunto: {subject}\\n\\n\"\n",
    "            f\"Pergunta: {row['Question']}\\n\"\n",
    "            f\"A) {row['A']}\\n\"\n",
    "            f\"B) {row['B']}\\n\"\n",
    "            f\"C) {row['C']}\\n\"\n",
    "            f\"D) {row['D']}\\n\\n\"\n",
    "            f\"Resposta correta:\"\n",
    "        )\n",
    "\n",
    "        return user_prompt\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Generation methods\n",
    "    # -----------------------------------------------------------------\n",
    "    def generate_response_with_chat_template(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the model for a given prompt using chat template.\n",
    "\n",
    "        Args:\n",
    "            prompt: The input prompt string\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the generated long and short text responses (continuation only)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        model_input = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Generation kwargs\n",
    "        generation_kwargs_long = {\n",
    "            \"max_new_tokens\": self.config.max_new_tokens_long,\n",
    "            \"do_sample\": self.config.do_sample_long,\n",
    "            \"temperature\": self.config.temperature,\n",
    "        }\n",
    "\n",
    "        generation_kwargs_short = {\n",
    "            \"max_new_tokens\": self.config.max_new_tokens_short,\n",
    "            \"do_sample\": self.config.do_sample_short,\n",
    "        }\n",
    "\n",
    "        # Generate long and short outputs\n",
    "        with torch.no_grad():\n",
    "            output_long = self.model.generate(\n",
    "                model_input, **generation_kwargs_long)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_short = self.model.generate(\n",
    "                model_input, **generation_kwargs_short)\n",
    "\n",
    "        # Decode only the continuation beyond the prompt\n",
    "        out_decoded_long = self.tokenizer.decode(\n",
    "            output_long[0][model_input.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        out_decoded_short = self.tokenizer.decode(\n",
    "            output_short[0][model_input.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        return out_decoded_long, out_decoded_short\n",
    "\n",
    "    def generate_response_seqlogprob(self, prompt: str, row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the model for a given prompt using sequential log-probabilities.\n",
    "\n",
    "        Args:\n",
    "            prompt: The input prompt string\n",
    "            row: DataFrame row containing question and options\n",
    "\n",
    "        Returns:\n",
    "            The predicted answer letter (A, B, C, or D)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        prefix_ids = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        options = {}\n",
    "        for letter in \"ABCD\":\n",
    "            target_text = f\"{letter}) {row[letter]}\"\n",
    "            target_ids = self.tokenizer(\n",
    "                target_text,\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\").to(self.device)[\"input_ids\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # feed prefix + target[:-1] to predict target[1:]\n",
    "                inp = torch.cat([prefix_ids, target_ids[:, :-1]], dim=1)\n",
    "                out = self.model(inp)\n",
    "                lp = torch.log_softmax(\n",
    "                    out.logits[:, -target_ids.shape[1]:, :], dim=-1)\n",
    "                # gather log-probs of the true next tokens\n",
    "                tgt = target_ids\n",
    "                token_lp = lp.gather(-1, tgt.unsqueeze(-1)).squeeze(-1)\n",
    "                seq_lp = token_lp.sum().item()\n",
    "            options[letter] = seq_lp\n",
    "\n",
    "        pred = max(options, key=options.get)\n",
    "        return pred\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Evaluation\n",
    "    # -----------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def extract_answer(text: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract the multiple choice answer letter from model output.\n",
    "\n",
    "        Args:\n",
    "            text: The model's generated text\n",
    "\n",
    "        Returns:\n",
    "            The extracted letter (a-d) in lowercase, or None if not found\n",
    "        \"\"\"\n",
    "        match = re.search(r'\\b([a-dA-D])\\b', text)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "        return None\n",
    "\n",
    "    def evaluate(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run the full evaluation pipeline.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with evaluation results\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"Model not loaded. Call load_model() first.\")\n",
    "\n",
    "        # Load test data\n",
    "        logger.info(f\"Loading test data from: {self.config.test_data_path}\")\n",
    "        df_test = pd.read_csv(self.config.test_data_path)\n",
    "\n",
    "        # Sample if configured\n",
    "        if self.config.sample_size:\n",
    "            df_test = df_test.sample(\n",
    "                n=min(self.config.sample_size, len(df_test)),\n",
    "                random_state=self.config.random_seed\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Evaluating on {len(df_test)} questions...\")\n",
    "\n",
    "        results = []\n",
    "        for _, row in tqdm(df_test.iterrows(), total=len(df_test), desc=\"Answering MMLU\"):\n",
    "            # Generate prompt\n",
    "            prompt = self.format_question(row)\n",
    "\n",
    "            # Generate responses\n",
    "            out_chat_long, out_chat_short = self.generate_response_with_chat_template(\n",
    "                prompt)\n",
    "            out_seqlogprob = self.generate_response_seqlogprob(prompt, row)\n",
    "\n",
    "            results.append({\n",
    "                \"subject\": row[\"Subject\"],\n",
    "                \"question\": row[\"Question\"],\n",
    "                \"A\": row[\"A\"],\n",
    "                \"B\": row[\"B\"],\n",
    "                \"C\": row[\"C\"],\n",
    "                \"D\": row[\"D\"],\n",
    "                \"correct\": row[\"Answer\"],\n",
    "                \"out_chat_long\": out_chat_long,\n",
    "                \"out_extracted_chat_long\": self.extract_answer(out_chat_long),\n",
    "                \"out_chat_short\": out_chat_short,\n",
    "                \"out_seqlogprob\": out_seqlogprob,\n",
    "            })\n",
    "\n",
    "        # Process results\n",
    "        self.results_df = pd.DataFrame(results)\n",
    "\n",
    "        return self.results_df\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Metrics\n",
    "    # -----------------------------------------------------------------\n",
    "    def save_results(self, output_path: Optional[Path] = None) -> None:\n",
    "        \"\"\"\n",
    "        Save evaluation results to CSV.\n",
    "\n",
    "        Args:\n",
    "            output_path: Optional custom output path. Uses config default if None.\n",
    "        \"\"\"\n",
    "        if self.results_df is None:\n",
    "            raise RuntimeError(\"No results to save. Run evaluate() first.\")\n",
    "\n",
    "        path = output_path or self.config.output_path\n",
    "        self.results_df.to_csv(path, index=False)\n",
    "        logger.info(f\"Results saved to: {path}\")\n",
    "\n",
    "    def _compute_accuracy(self, col: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute accuracy for a given output column.\n",
    "\n",
    "        Args:\n",
    "            col: Column name in results_df to evaluate\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with accuracy and other metrics\n",
    "        \"\"\"\n",
    "        df = self.results_df\n",
    "        correct = (df[col].str.upper() == df[\"correct\"].str.upper()).sum()\n",
    "        total = len(df)\n",
    "        empty = df[col].isna().sum()\n",
    "        return {\n",
    "            \"accuracy\": 100 * correct / total,\n",
    "            \"correct\": correct,\n",
    "            \"total\": total,\n",
    "            \"empty\": empty,\n",
    "            \"distribution\": df[col].value_counts().to_dict(),\n",
    "        }\n",
    "\n",
    "    def calculate_metrics(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with metrics for each evaluation mode\n",
    "        \"\"\"\n",
    "        if self.results_df is None:\n",
    "            raise RuntimeError(\"No results to compute metrics on.\")\n",
    "        return {\n",
    "            \"with_chat_long_answer\": self._compute_accuracy(\"out_extracted_chat_long\"),\n",
    "            \"with_chat_short_answer\": self._compute_accuracy(\"out_chat_short\"),\n",
    "            \"seqlogprob\": self._compute_accuracy(\"out_seqlogprob\"),\n",
    "        }\n",
    "\n",
    "    def print_metrics(self) -> None:\n",
    "        \"\"\"Print evaluation metrics in a formatted way.\"\"\"\n",
    "        metrics = self.calculate_metrics()\n",
    "        for mode, data in metrics.items():\n",
    "            logger.info(f\"\\n{'='*50}\\nRESULTS ({mode})\\n{'='*50}\")\n",
    "            logger.info(f\"Accuracy: {data['accuracy']:.2f}% \"\n",
    "                        f\"({data['correct']}/{data['total']})\")\n",
    "            logger.info(f\"Empty answers: {data['empty']}\")\n",
    "            logger.info(f\"Distribution: {data['distribution']}\")\n",
    "\n",
    "    def run(self) -> dict:\n",
    "        \"\"\"\n",
    "        Convenience method to run the complete evaluation pipeline.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            f\"\\n{'='*50}\\nEVALUATION ({self.config.model_name})\\n{'='*50}\")\n",
    "        if self.config.lora:\n",
    "            self.load_model_lora()\n",
    "        else:\n",
    "            self.load_model()\n",
    "        self.evaluate()\n",
    "        self.save_results()\n",
    "        self.print_metrics()\n",
    "\n",
    "        return self.calculate_metrics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc6ce7",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation Runs <a id=\"part_03\"></a>\n",
    "\n",
    "This section executes the evaluation pipeline on three different *Gemma-3-1b* configurations.\n",
    "Each run loads the corresponding model (with or without LoRA adapters), evaluates it against the\n",
    "*MMLU Portuguese test set*, and saves results to the `results/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382a45d",
   "metadata": {},
   "source": [
    "### 3.1 *Gemma-3-1b-it* (Baseline) <a id=\"31-gemma-3-1b-it\"></a>\n",
    "\n",
    "This experiment evaluates the **instruction-tuned baseline model** (`gemma-3-1b-it`) without any fine-tuning.\n",
    "It serves as a reference point to assess the impact of domain-specific adaptation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f4de4c",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning:** The next Python cell involves **high-performance computing (HPC)**.  \n",
    "> Execution requires a **dedicated or cloud machine with multiple cores**, not a standard desktop or notebook.  \n",
    "> Runtime and cell outputs are reported below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EvaluationConfig(\n",
    "    model_path=\"google/gemma-3-1b-it\",\n",
    "    lora=False\n",
    ")\n",
    "\n",
    "evaluator = MMLUEvaluator(config)\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d19dfc",
   "metadata": {},
   "source": [
    "```log\n",
    "2025-11-25 04:16:41.833 | INFO     | __main__:run:435 - \n",
    "==================================================\n",
    "EVALUATION (models_gemma-3-1b-it)\n",
    "==================================================\n",
    "2025-11-25 04:16:41.834 | INFO     | __main__:load_model:129 - Device: cuda\n",
    "2025-11-25 04:16:41.834 | INFO     | __main__:load_model:130 - Loading model: models/gemma-3-1b-it\n",
    "2025-11-25 04:16:44.174 | INFO     | __main__:load_model:139 - Model loaded successfully\n",
    "2025-11-25 04:16:44.174 | INFO     | __main__:evaluate:323 - Loading test data from: data/mmlu_test.csv\n",
    "2025-11-25 04:16:44.183 | INFO     | __main__:evaluate:333 - Evaluating on 1038 questions...\n",
    "2025-11-25 04:28:38.244 | INFO     | __main__:save_results:379 - Results saved to: results/models_gemma-3-1b-it_results.csv\n",
    "2025-11-25 04:28:38.248 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (with_chat_long_answer)\n",
    "==================================================\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:423 - Accuracy: 30.64% (318/1038)\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:426 - Distribution: {'C': 633, 'A': 200, 'B': 110, 'D': 95}\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (with_chat_short_answer)\n",
    "==================================================\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:423 - Accuracy: 30.64% (318/1038)\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:426 - Distribution: {'C': 636, 'A': 197, 'B': 109, 'D': 93, 'The': 3}\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (seqlogprob)\n",
    "==================================================\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:423 - Accuracy: 30.35% (315/1038)\n",
    "2025-11-25 04:28:38.249 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 04:28:38.250 | INFO     | __main__:print_metrics:426 - Distribution: {'C': 603, 'A': 200, 'B': 141, 'D': 94}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af3d223",
   "metadata": {},
   "source": [
    "### 3.2 *Gemma-3-1b-pt* + LoRA SFT on MMLU <a id=\"32-gemma-3-1b-pt--lora-sft-on-mmlu\"></a>\n",
    "\n",
    "This configuration applies **LoRA-based supervised fine-tuning (SFT)** to the pre-trained base model `gemma-3-1b-pt`,\n",
    "trained specifically on the MMLU dataset.\n",
    "It measures how well task-specific adaptation improves factual and reasoning performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1b005",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning:** The next Python cell involves **high-performance computing (HPC)**.  \n",
    "> Execution requires a **dedicated or cloud machine with multiple cores**, not a standard desktop or notebook.  \n",
    "> Runtime and cell outputs are reported below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27fa939",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EvaluationConfig(\n",
    "    model_path=\"models/gemma-3-1b-pt-sft\",\n",
    "    base_model_path=\"google/gemma-3-1b-pt\",\n",
    "    lora=True\n",
    ")\n",
    "\n",
    "evaluator = MMLUEvaluator(config)\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b344f35",
   "metadata": {},
   "source": [
    "```log\n",
    "2025-11-25 04:28:38.312 | INFO     | __main__:run:435 - \n",
    "==================================================\n",
    "EVALUATION (gemma-3-1b-pt-sft105_best_eval)\n",
    "==================================================\n",
    "2025-11-25 04:28:38.312 | INFO     | __main__:load_model_lora:143 - Device: cuda\n",
    "2025-11-25 04:28:38.312 | INFO     | __main__:load_model_lora:144 - Loading BASE MODEL + LORA ADAPTER\n",
    "2025-11-25 04:28:38.312 | INFO     | __main__:load_model_lora:147 - Base model: models/gemma-3-1b-pt\n",
    "2025-11-25 04:28:38.892 | INFO     | __main__:load_model_lora:156 - Loading LoRA adapter: old/models/gemma-3-1b-pt-sft105/best_eval\n",
    "2025-11-25 04:28:43.528 | INFO     | __main__:load_model_lora:171 - Model + LoRA loaded successfully on cuda\n",
    "2025-11-25 04:28:43.528 | INFO     | __main__:evaluate:323 - Loading test data from: data/mmlu_test.csv\n",
    "2025-11-25 04:28:43.537 | INFO     | __main__:evaluate:333 - Evaluating on 1038 questions...\n",
    "2025-11-25 05:35:18.616 | INFO     | __main__:save_results:379 - Results saved to: results/gemma-3-1b-pt-sft105_best_eval_results.csv\n",
    "2025-11-25 05:35:18.619 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (with_chat_long_answer)\n",
    "==================================================\n",
    "2025-11-25 05:35:18.619 | INFO     | __main__:print_metrics:423 - Accuracy: 24.95% (259/1038)\n",
    "2025-11-25 05:35:18.620 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 05:35:18.620 | INFO     | __main__:print_metrics:426 - Distribution: {'C': 301, 'B': 258, 'A': 246, 'D': 233}\n",
    "2025-11-25 05:35:18.620 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (with_chat_short_answer)\n",
    "==================================================\n",
    "2025-11-25 05:35:18.620 | INFO     | __main__:print_metrics:423 - Accuracy: 27.94% (290/1038)\n",
    "2025-11-25 05:35:18.620 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 05:35:18.620 | INFO     | __main__:print_metrics:426 - Distribution: {'C': 435, 'A': 277, 'D': 207, 'B': 116, 'Você': 3}\n",
    "2025-11-25 05:35:18.621 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (seqlogprob)\n",
    "==================================================\n",
    "2025-11-25 05:35:18.621 | INFO     | __main__:print_metrics:423 - Accuracy: 25.63% (266/1038)\n",
    "2025-11-25 05:35:18.621 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 05:35:18.621 | INFO     | __main__:print_metrics:426 - Distribution: {'C': 342, 'B': 248, 'A': 230, 'D': 218}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b2b5c",
   "metadata": {},
   "source": [
    "### 3.3 *Gemma-3-1b-pt* + Wiki Context + LoRA SFT on MMLU <a id=\"33-gemma-3-1b-pt--wiki-context--lora-sft-on-mmlu\"></a>\n",
    "\n",
    "In this final evaluation, the model integrates **contextual knowledge** from the curated\n",
    "Wikipedia subset (*Law, Governance, and Ethics*) into its LoRA fine-tuning process.\n",
    "This variant assesses whether combining factual domain data with reasoning QA pairs\n",
    "enhances zero-shot performance on MMLU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c48dcd2",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning:** The next Python cell involves **high-performance computing (HPC)**.  \n",
    "> Execution requires a **dedicated or cloud machine with multiple cores**, not a standard desktop or notebook.  \n",
    "> Runtime and cell outputs are reported below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EvaluationConfig(\n",
    "    model_path=\"AleNunezArroyo/gemma-3-1b-pt-contextual-e1-ckpt1600\",\n",
    "    base_model_path=\"google/gemma-3-1b-pt\",\n",
    "    lora=True\n",
    ")\n",
    "\n",
    "evaluator = MMLUEvaluator(config)\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062f96d",
   "metadata": {},
   "source": [
    "```log\n",
    "2025-11-25 08:12:24.251 | INFO     | __main__:run:435 - \n",
    "==================================================\n",
    "EVALUATION (gemma-3-1b-pt-contextual-e1-ckpt1600-sft115_best_eval)\n",
    "==================================================\n",
    "2025-11-25 08:12:24.252 | INFO     | __main__:load_model_lora:143 - Device: cuda\n",
    "2025-11-25 08:12:24.252 | INFO     | __main__:load_model_lora:144 - Loading BASE MODEL + LORA ADAPTER\n",
    "2025-11-25 08:12:24.252 | INFO     | __main__:load_model_lora:147 - Base model: models/gemma-3-1b-pt\n",
    "2025-11-25 08:12:24.930 | INFO     | __main__:load_model_lora:156 - Loading LoRA adapter: models/gemma-3-1b-pt-contextual-e1-ckpt1600-sft115/best_eval\n",
    "2025-11-25 08:12:26.649 | INFO     | __main__:load_model_lora:171 - Model + LoRA loaded successfully on cuda\n",
    "2025-11-25 08:12:26.649 | INFO     | __main__:evaluate:323 - Loading test data from: data/mmlu_test.csv\n",
    "2025-11-25 08:12:26.659 | INFO     | __main__:evaluate:333 - Evaluating on 1038 questions...\n",
    "2025-11-25 09:30:42.335 | INFO     | __main__:save_results:379 - Results saved to: results/gemma-3-1b-pt-contextual-e1-ckpt1600-sft115_best_eval_results.csv\n",
    "2025-11-25 09:30:42.338 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (with_chat_long_answer)\n",
    "==================================================\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:423 - Accuracy: 26.30% (273/1038)\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:426 - Distribution: {'B': 327, 'D': 288, 'C': 264, 'A': 159}\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (with_chat_short_answer)\n",
    "==================================================\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:423 - Accuracy: 26.01% (270/1038)\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:426 - Distribution: {'B': 433, 'D': 322, 'C': 276, 'A': 7}\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:422 - \n",
    "==================================================\n",
    "RESULTS (seqlogprob)\n",
    "==================================================\n",
    "2025-11-25 09:30:42.339 | INFO     | __main__:print_metrics:423 - Accuracy: 25.92% (269/1038)\n",
    "2025-11-25 09:30:42.340 | INFO     | __main__:print_metrics:425 - Empty answers: 0\n",
    "2025-11-25 09:30:42.340 | INFO     | __main__:print_metrics:426 - Distribution: {'B': 526, 'D': 340, 'C': 165, 'A': 7}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-2s2025-ssrl-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
