{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba2a47f",
   "metadata": {},
   "source": [
    "### Project: Domain Adaptation of Portuguese SLMs via Self-Supervised Fine-Tuning with LoRA\n",
    "MO436C - Introduction to Self-Supervised Learning (SSRL)\n",
    "\n",
    "**Team Members:**\n",
    "- Alejandro Núñez Arroyo. <a href=\"mailto:a299215@dac.unicamp.br\">a299215@dac.unicamp.br</a>  \n",
    "- Daniel da Costa Nunes Resende Neto. <a href=\"mailto:d169408@dac.unicamp.br\">d169408@dac.unicamp.br</a>  \n",
    "- José Augusto de Almeida Neto. <a href=\"mailto:j299218@dac.unicamp.br\">j299218@dac.unicamp.br</a>  \n",
    "\n",
    "*Instituto de Computação (IC), Universidade Estadual de Campinas (UNICAMP)*  \n",
    "*Campinas, November 2025*\n",
    "\n",
    "---\n",
    "\n",
    "#### License\n",
    "\n",
    "This notebook and its source code are released under the **GNU General Public License v3.0 (GPLv3)**.  \n",
    "You are free to use, modify, and redistribute this work under the following terms:\n",
    "\n",
    "> **GNU General Public License v3.0**  \n",
    "> Copyright © 2025 The Authors listed above  \n",
    ">\n",
    "> This program is free software: you can redistribute it and/or modify  \n",
    "> it under the terms of the GNU General Public License as published by  \n",
    "> the Free Software Foundation, either version 3 of the License, or  \n",
    "> (at your option) any later version.  \n",
    ">\n",
    "> This program is distributed in the hope that it will be useful,  \n",
    "> but **without any warranty**; without even the implied warranty of  \n",
    "> merchantability or fitness for a particular purpose. See the  \n",
    "> GNU General Public License for more details.  \n",
    ">\n",
    "> You should have received a copy of the GNU General Public License  \n",
    "> along with this program. If not, see  \n",
    "> [https://www.gnu.org/licenses/gpl-3.0.en.html](https://www.gnu.org/licenses/gpl-3.0.en.html).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f2d7b",
   "metadata": {},
   "source": [
    "# Notebook 3b: Supervised Fine-Tuning (Instruction Tuning on  Wiki Context model)\n",
    "\n",
    "This notebook documents the **supervised fine-tuning (SFT)** stage of the experimental pipeline,  \n",
    "where the **contextually pre-trained model** `gemma-3-1b-pt-contextual-e1-ckpt1600` — obtained after continued self-supervised training on the Portuguese Wikipedia subset (*Law, Governance, and Ethics*) —  \n",
    "is adapted to the **MMLU multiple-choice question answering task** using **Low-Rank Adaptation (LoRA)**.  \n",
    "\n",
    "This corresponds to the **Context-Adapted + Instruction-Tuned Model** described in the experimental report —  \n",
    "representing the *third* model variant, which combines **domain adaptation** (from the SSRL stage)  \n",
    "with **task-specific supervised fine-tuning** on MMLU.\n",
    "\n",
    "---\n",
    "\n",
    "**Overview**\n",
    "\n",
    "The main objectives of this notebook are:\n",
    "\n",
    "1. **Setup & Environment Initialization**  \n",
    "   Load all necessary dependencies, configure GPU acceleration, and prepare model and dataset paths.\n",
    "\n",
    "2. **Data Preparation**  \n",
    "   - Load and explore the *MMLU Portuguese training dataset* (`mmlu_train.csv`).  \n",
    "   - Perform stratified splits into train and validation subsets.  \n",
    "   - Convert the data into **instruction-tuning format** following the *Gemma 3 chat schema*.\n",
    "\n",
    "3. **LoRA Configuration & SFT Setup**  \n",
    "   - Define the LoRA adaptation modules and parameters (`r`, `alpha`, `dropout`).  \n",
    "   - Configure the SFT trainer and optimization strategy using the **Hugging Face TRL** framework.  \n",
    "   - Run pre-training sanity checks to validate dataset tokenization, masking, and loss targets.\n",
    "\n",
    "4. **Model Training & Saving**  \n",
    "   - Execute the full supervised fine-tuning routine.  \n",
    "   - Log metrics such as training loss, token accuracy, and gradient norms.  \n",
    "   - Save the resulting adapter weights and tokenizer for downstream evaluation.\n",
    "\n",
    "**Output Artifacts**\n",
    "   - `gemma-3-1b-pt-contextual-e1-ckpt1600-sft/` — directory containing the new **instruction LoRA adapters** trained on top of the contextual model.  \n",
    "   - `trainer_state.json` — logs of loss, accuracy, and gradient statistics.  \n",
    "   - Model checkpoints saved every 50 steps.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5de111",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* [Part 1: Setup & Imports](#1-setup--imports)\n",
    "  - [1.1 Load Data](#11-load-data)\n",
    "  - [1.2 Load Model](#12-load-model)\n",
    "* [Part 2: Data](#2-data)\n",
    "  - [2.1 Prepare Data](#21-prepare-data)\n",
    "  - [2.2 Convert to Instruction-Tuning Format](#22-convert-to-instruction-tuning-format)\n",
    "* [Part 3: LoRA Instruction-Tuning](#3-lora-instruction-tuning)\n",
    "  - [3.1 LoRA Configuration](#31-lora-configuration)\n",
    "  - [3.2 Training Setup](#32-training-setup)\n",
    "  - [3.3 Sanity Checks](#33-sanity-checks)\n",
    "  - [3.4 Training](#34-training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e5740",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "This section initializes the working environment, importing all required libraries for dataset handling,  \n",
    "model loading, fine-tuning, and monitoring. It also verifies CUDA availability and frees GPU memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252c6ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "Garbage collection done and CUDA cache emptied.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Garbage collection done and CUDA cache emptied.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7bda11",
   "metadata": {},
   "source": [
    "After confirming system readiness, a fixed **random seed** ensures reproducibility.\n",
    "Here, two model paths are defined:\n",
    "\n",
    "* the **original base model** (`gemma-3-1b-pt`) and\n",
    "* the **contextual adapter** (`gemma-3-1b-pt-contextual-e1-ckpt1600`)\n",
    "  which will be **merged** before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3844cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "RANDOM_SEED = 27\n",
    "\n",
    "# Model paths\n",
    "BASE_MODEL_ID = \"../models/gemma-3-1b-pt-contextual-e1-ckpt1600\"\n",
    "BASE_MODEL_ORIGINAL_ID = \"../models/gemma-3-1b-pt\"\n",
    "BASE_MODEL_FOR_CHAT_TEMPLATE = \"../models/gemma-3-1b-it\"\n",
    "\n",
    "# Directories and file paths\n",
    "TRAIN_PATH = Path(\"../../data/mmlu_train.csv\")\n",
    "OUTPUT_MODEL_DIR = Path(f\"{BASE_MODEL_ID}-sft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535473e8",
   "metadata": {},
   "source": [
    "### 1.1 Load Data <a id=\"11-load-data\"></a>\n",
    "\n",
    "Load the **Portuguese MMLU training dataset** used for instruction fine-tuning.\n",
    "Basic exploratory statistics verify subject balance and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec15048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN SET OVERVIEW - Total: 2,419 samples\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SUBJECT DISTRIBUTION - Total subjects: 7\n",
      "============================================================\n",
      "Subject\n",
      "professional_law     1073\n",
      "moral_scenarios       626\n",
      "moral_disputes        242\n",
      "philosophy            218\n",
      "logical_fallacies     114\n",
      "jurisprudence          76\n",
      "business_ethics        70\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "ANSWER DISTRIBUTION - Total answers: 4\n",
      "============================================================\n",
      "Answer\n",
      "C    639\n",
      "B    610\n",
      "D    590\n",
      "A    580\n",
      "Name: count, dtype: int64\n",
      "- With this distribution, there's no strong statistical incentive to always answer a specific letter.\n",
      "\n",
      "Duplicate questions in train: 0\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "# Explore data\n",
    "print(\"=\" * 60)\n",
    "print(f\"TRAIN SET OVERVIEW - Total: {len(df_train):,} samples\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Subject and Answer distributions\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "subject_counts = df_train['Subject'].value_counts()\n",
    "print(f\"SUBJECT DISTRIBUTION - Total subjects: {len(subject_counts)}\")\n",
    "print(\"=\" * 60)\n",
    "print(subject_counts)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "answer_counts = df_train['Answer'].value_counts()\n",
    "print(f\"ANSWER DISTRIBUTION - Total answers: {len(answer_counts)}\")\n",
    "print(\"=\" * 60)\n",
    "print(answer_counts)\n",
    "print(\"- With this distribution, there's no strong statistical incentive to always answer a specific letter.\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_train.duplicated(subset=['Question']).sum()\n",
    "print(f\"\\nDuplicate questions in train: {duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9febff",
   "metadata": {},
   "source": [
    "#### Dataset Summary\n",
    "\n",
    "* **Total Samples:** 2,419 unique question–answer pairs.\n",
    "* **Main Domains:** Law, Ethics, and Philosophy — mirroring the project’s focus on\n",
    "  *Law, Governance, and Ethics* macrodomain.\n",
    "* **Duplicate Check:** 0 duplicates found.\n",
    "\n",
    "| Subject           | Count |\n",
    "| :---------------- | :---- |\n",
    "| professional_law  | 1,073 |\n",
    "| moral_scenarios   | 626   |\n",
    "| moral_disputes    | 242   |\n",
    "| philosophy        | 218   |\n",
    "| logical_fallacies | 114   |\n",
    "| jurisprudence     | 76    |\n",
    "| business_ethics   | 70    |\n",
    "\n",
    "Answer Distribuition\n",
    "| Answer | Count |\n",
    "| :----- | :---- |\n",
    "| C      | 639   |\n",
    "| B      | 610   |\n",
    "| D      | 590   |\n",
    "| A      | 580   |\n",
    "\n",
    "This balanced answer distribution ensures the model **must rely on semantic understanding**,\n",
    "not frequency bias, to achieve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29db28b",
   "metadata": {},
   "source": [
    "### 1.2 Load Model <a id=\"12-load-model\"></a>\n",
    "\n",
    "For this experiment, we must **merge the contextual adapter** (obtained from SSRL on Wikipedia PT-BR)\n",
    "into the original `gemma-3-1b-pt` base weights before starting SFT.\n",
    "\n",
    "The merge-and-unload process permanently fuses the domain-adapted LoRA into the model,\n",
    "yielding a **single contextualized transformer** ready for new instruction adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73077f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_sequential(base_model_id, adapter_path):\n",
    "    \"\"\"\n",
    "    1. Loads Base Model (Gemma-3-1b-pt)\n",
    "    2. Loads Contextual Adapter\n",
    "    3. Merges Adapter into Base\n",
    "    4. Unloads Adapter to free resources\n",
    "    5. Returns a 'clean' merged model ready for a NEW LoRA\n",
    "    \"\"\"\n",
    "    print(f\"--- STRATEGY: MERGE & RE-LORA ---\")\n",
    "    print(f\"1. Loading Base: {base_model_id}\")\n",
    "    \n",
    "    # Load Base\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_id,\n",
    "        dtype=torch.bfloat16, # 40-series GPUs love bfloat16\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    print(f\"2. Loading Context Adapter: {adapter_path}\")\n",
    "    # Load the Contextual Adapter\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    \n",
    "    print(\"3. Merging Context weights into Base Model...\")\n",
    "    # THE CRITICAL STEP: Merge weights permanently\n",
    "    model = model.merge_and_unload()\n",
    "    \n",
    "    # Verify we are back to a standard model architecture\n",
    "    print(f\"Model Type after merge: {type(model)}\")\n",
    "    \n",
    "    # Load Tokenizer (Usually from the base is fine, unless you added tokens)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STRATEGY: MERGE & RE-LORA ---\n",
      "1. Loading Base: ../../models/gemma-3-1b-pt\n",
      "2. Loading Context Adapter: ../../models/gemma-3-1b-pt-contextual-e1-ckpt1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeneto/projects/msc-2s2025_SSRL/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:693: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Merging Context weights into Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeneto/projects/msc-2s2025_SSRL/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:424: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM\n",
      "\n",
      "# Load original tied model\n",
      "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
      "\n",
      "# Set the randomly initialized lm_head to the previously tied embeddings\n",
      "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
      "\n",
      "# Save the untied model\n",
      "untied_model_dir = \"dir/for/untied/model\"\n",
      "model.save_pretrained(untied_model_dir)\n",
      "model.config.save_pretrained(untied_model_dir)\n",
      "\n",
      "# Now use the original model but in untied format\n",
      "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
      "```\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type after merge: <class 'transformers.models.gemma3.modeling_gemma3.Gemma3ForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "tokenizer, model = load_model_sequential(BASE_MODEL_ORIGINAL_ID, BASE_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdc3ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL INFORMATION\n",
      "============================================================\n",
      "Model type: <class 'transformers.models.gemma3.modeling_gemma3.Gemma3ForCausalLM'>\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.bfloat16\n",
      "Tokenizer pad token: <pad>\n",
      "Model config pad token id: 0\n",
      "Model has existing PEFT adapters\n",
      "PEFT config: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping={'base_model_class': 'Gemma3ForCausalLM', 'parent_library': 'transformers.models.gemma3.modeling_gemma3', 'unsloth_fixed': True}, base_model_name_or_path='/mnt/shared_models/_home/aarroyo/models/gemma-3-1b-pt', revision=None, inference_mode=True, r=128, target_modules={'embed_tokens', 'down_proj', 'gate_proj', 'k_proj', 'lm_head', 'o_proj', 'q_proj', 'up_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0, fan_in_fan_out=False, bias='none', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify model loading and print some information\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")\n",
    "print(f\"Model config pad token id: {model.config.pad_token_id}\")\n",
    "\n",
    "# Check if the model has existing PEFT adapters\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        print(\"Model has existing PEFT adapters\")\n",
    "        print(f\"PEFT config: {model.peft_config}\")\n",
    "    else:\n",
    "        print(\"Model does not have existing PEFT adapters\")\n",
    "except:\n",
    "    print(\"PEFT status unknown\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d2234",
   "metadata": {},
   "source": [
    "**Model Summary**\n",
    "\n",
    "* **Architecture:** Gemma 3 1B (Portuguese) — contextually pre-trained\n",
    "* **Precision:** bfloat16\n",
    "* **Adapter Layers:** Merged (Wiki context permanently integrated)\n",
    "* **Tokenizer:** Compatible with `<eos>` as padding token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2e45d",
   "metadata": {},
   "source": [
    "## 2. Data <a id=\"part_02\"></a>\n",
    "\n",
    "This stage prepares the dataset for supervised fine-tuning by splitting,\n",
    "formatting, and converting it into the **Gemma-compatible chat format**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807ebc5",
   "metadata": {},
   "source": [
    "### 2.1 Prepare Data <a id=\"21-prepare-data\"></a>\n",
    "\n",
    "A **stratified 90/10 split** preserves subject balance between training and validation sets.\n",
    "\n",
    "| Subset     | Samples |\n",
    "| :--------- | :------ |\n",
    "| Train      | 2,177   |\n",
    "| Validation | 242     |\n",
    "\n",
    "This structure ensures each subject domain contributes proportionally during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c494ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2177  |  Val: 242\n",
      "\n",
      "Train set subject distribution:\n",
      "Subject\n",
      "professional_law     966\n",
      "moral_scenarios      563\n",
      "moral_disputes       218\n",
      "philosophy           196\n",
      "logical_fallacies    103\n",
      "jurisprudence         68\n",
      "business_ethics       63\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation set subject distribution:\n",
      "Subject\n",
      "professional_law     107\n",
      "moral_scenarios       63\n",
      "moral_disputes        24\n",
      "philosophy            22\n",
      "logical_fallacies     11\n",
      "jurisprudence          8\n",
      "business_ethics        7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into train and validation sets\n",
    "train_df, val_df = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df_train['Subject']\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)}  |  Val: {len(val_df)}\")\n",
    "\n",
    "subject_counts = train_df['Subject'].value_counts()\n",
    "print(f\"\\nTrain set subject distribution:\")\n",
    "print(subject_counts)\n",
    "subject_counts = val_df['Subject'].value_counts()\n",
    "print(f\"\\nValidation set subject distribution:\")\n",
    "print(subject_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ccc4f",
   "metadata": {},
   "source": [
    "### 2.2 Convert to Instruction-Tuning Format <a id=\"22-convert-to-instruction-tuning-format\"></a>\n",
    "\n",
    "The dataset is reformatted into conversational structure following the *Gemma 3 chat schema*.\n",
    "Each entry becomes a **(user, assistant)** message pair, where:\n",
    "\n",
    "* The **user prompt** includes system instructions and the multiple-choice question.\n",
    "* The **assistant response** provides the correct letter and answer text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da380030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_user_prompt(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Build the user prompt for a given row in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # System instruction\n",
    "    system_instruction = (\n",
    "        \"Você é um assistente especialista que responde questões de múltipla escolha em português do Brasil.\\n\"\n",
    "        \"Responda apenas com UMA opção correta (A, B, C ou D).\\n\"\n",
    "    )\n",
    "\n",
    "    # Input - User turn\n",
    "    subject = row['Subject'].replace('_', ' ').title()\n",
    "    \n",
    "    return (\n",
    "        f\"{system_instruction}\"\n",
    "        f\"Assunto: {subject}\\n\\n\"\n",
    "        f\"Pergunta: {row['Question']}\\n\"\n",
    "        f\"A) {row['A']}\\n\"\n",
    "        f\"B) {row['B']}\\n\"\n",
    "        f\"C) {row['C']}\\n\"\n",
    "        f\"D) {row['D']}\\n\\n\"\n",
    "        \"Resposta correta:\" \n",
    "    )\n",
    "\n",
    "def format_gemma_instruction(row: pd.Series) -> dict:\n",
    "    \"\"\"\n",
    "    Convert MMLU row to Gemma 3 chat format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input - User turn\n",
    "    user_content = build_user_prompt(row)\n",
    "\n",
    "    # Output - Model turn\n",
    "    answer = row['Answer'] \n",
    "    model_content = f\"{answer}) {row[answer]}\"\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": model_content}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "201f3a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5980a381965b48659ea14213305c7abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2177 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963ff1433a354953a32dc06c44a16487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'Você é um assistente especialista que responde questões de múltipla escolha em português do Brasil.\\nResponda apenas com UMA opção correta (A, B, C ou D).\\nAssunto: Business Ethics\\n\\nPergunta: Os gerentes têm a confiança de administrar a empresa aos melhores interesses dos _______. Especificamente, eles têm o dever de agir ao benefício da empresa, além do dever de __________ e de ____________.\\nA) Acionistas, Cuidado e habilidade, Diligência.\\nB) Partes interessadas, cuidado e habilidade, Diligência\\nC) Acionistas, Interesse próprio, Diligência\\nD) Acionistas, Cuidado e habilidade, Interesse próprio\\n\\nResposta correta:', 'role': 'user'}, {'content': 'A) Acionistas, Cuidado e habilidade, Diligência.', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Apply formatting\n",
    "train_dataset = Dataset.from_pandas(train_df).map(format_gemma_instruction)\n",
    "val_dataset = Dataset.from_pandas(val_df).map(format_gemma_instruction)\n",
    "\n",
    "# Remove columns we don't need so TRL doesn't get confused\n",
    "columns_to_keep = [\"messages\"]\n",
    "train_dataset = train_dataset.remove_columns([c for c in train_dataset.column_names if c not in columns_to_keep])\n",
    "val_dataset = val_dataset.remove_columns([c for c in val_dataset.column_names if c not in columns_to_keep])\n",
    "\n",
    "# Check the formatted text\n",
    "print(train_dataset[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe2f974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ bos_token }}\n",
      "{%- if messages[0]['role'] == 'system' -%}\n",
      "    {%- if messages[0]['content'] is string -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- else -%}\n",
      "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
      "\n",
      "' -%}\n",
      "    {%- endif -%}\n",
      "    {%- set loop_messages = messages[1:] -%}\n",
      "{%- else -%}\n",
      "    {%- set first_user_prefix = \"\" -%}\n",
      "    {%- set loop_messages = messages -%}\n",
      "{%- endif -%}\n",
      "{%- for message in loop_messages -%}\n",
      "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
      "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
      "    {%- endif -%}\n",
      "    {%- if (message['role'] == 'assistant') -%}\n",
      "        {%- set role = \"model\" -%}\n",
      "    {%- else -%}\n",
      "        {%- set role = message['role'] -%}\n",
      "    {%- endif -%}\n",
      "    {{ '<start_of_turn>' + role + '\n",
      "' + (first_user_prefix if loop.first else \"\") }}\n",
      "    {%- if message['content'] is string -%}\n",
      "        {{ message['content'] | trim }}\n",
      "    {%- elif message['content'] is iterable -%}\n",
      "        {%- for item in message['content'] -%}\n",
      "            {%- if item['type'] == 'image' -%}\n",
      "                {{ '<start_of_image>' }}\n",
      "            {%- elif item['type'] == 'text' -%}\n",
      "                {{ item['text'] | trim }}\n",
      "            {%- endif -%}\n",
      "        {%- endfor -%}\n",
      "    {%- else -%}\n",
      "        {{ raise_exception(\"Invalid content type\") }}\n",
      "    {%- endif -%}\n",
      "    {{ '<end_of_turn>\n",
      "' }}\n",
      "{%- endfor -%}\n",
      "{%- if add_generation_prompt -%}\n",
      "    {{'<start_of_turn>model\n",
      "'}}\n",
      "{%- endif -%}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Gemma 3 chat template from \"gemma-3-1b-it\"\n",
    "tokenizer_it = AutoTokenizer.from_pretrained(BASE_MODEL_FOR_CHAT_TEMPLATE)\n",
    "\n",
    "print(tokenizer_it.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eec2eb",
   "metadata": {},
   "source": [
    "The `chat_template_with_gen` defines the conversational control tokens (`<start_of_turn>`, `<bos>`, `<end_of_turn>`)\n",
    "ensuring TRL interprets message boundaries correctly.\n",
    "\n",
    "A small sample is inspected to visually confirm the final training input structure before tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d2778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Você é um assistente especialista que responde questões de múltipla escolha em português do Brasil.\n",
      "Responda apenas com UMA opção correta (A, B, C ou D).\n",
      "Assunto: Business Ethics\n",
      "\n",
      "Pergunta: Os gerentes têm a confiança de administrar a empresa aos melhores interesses dos _______. Especificamente, eles têm o dever de agir ao benefício da empresa, além do dever de __________ e de ____________.\n",
      "A) Acionistas, Cuidado e habilidade, Diligência.\n",
      "B) Partes interessadas, cuidado e habilidade, Diligência\n",
      "C) Acionistas, Interesse próprio, Diligência\n",
      "D) Acionistas, Cuidado e habilidade, Interesse próprio\n",
      "\n",
      "Resposta correta:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "A) Acionistas, Cuidado e habilidade, Diligência.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add generation keyword - required for TRL to know when to start generating\n",
    "chat_template_with_gen =\"\"\"{{ bos_token }}\n",
    "{%- if messages[0]['role'] == 'system' -%}\n",
    "    {%- if messages[0]['content'] is string -%}\n",
    "        {%- set first_user_prefix = messages[0]['content'] + '\n",
    "\n",
    "' -%}\n",
    "    {%- else -%}\n",
    "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n",
    "\n",
    "' -%}\n",
    "    {%- endif -%}\n",
    "    {%- set loop_messages = messages[1:] -%}\n",
    "{%- else -%}\n",
    "    {%- set first_user_prefix = \"\" -%}\n",
    "    {%- set loop_messages = messages -%}\n",
    "{%- endif -%}\n",
    "{%- for message in loop_messages -%}\n",
    "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
    "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
    "    {%- endif -%}\n",
    "    {%- if (message['role'] == 'assistant') -%}\n",
    "        {%- set role = \"model\" -%}\n",
    "    {%- else -%}\n",
    "        {%- set role = message['role'] -%}\n",
    "    {%- endif -%}\n",
    "    {{ '<start_of_turn>' + role + '\n",
    "' + (first_user_prefix if loop.first else \"\") }}\n",
    "    {%- if message['content'] is string -%}\n",
    "        {% generation %}{{ message['content'] | trim }}{% endgeneration %}\n",
    "    {%- elif message['content'] is iterable -%}\n",
    "        {%- for item in message['content'] -%}\n",
    "            {%- if item['type'] == 'image' -%}\n",
    "                {{ '<start_of_image>' }}\n",
    "            {%- elif item['type'] == 'text' -%}\n",
    "                {{ item['text'] | trim }}\n",
    "            {%- endif -%}\n",
    "        {%- endfor -%}\n",
    "    {%- else -%}\n",
    "        {{ raise_exception(\"Invalid content type\") }}\n",
    "    {%- endif -%}\n",
    "    {{ '<end_of_turn>\n",
    "' }}\n",
    "{%- endfor -%}\n",
    "{%- if add_generation_prompt -%}\n",
    "    {{'<start_of_turn>model\n",
    "'}}\n",
    "{%- endif -%}\n",
    "\"\"\"\n",
    "\n",
    "# \"Copy\" Gemma 3 chat template\n",
    "tokenizer.chat_template = chat_template_with_gen\n",
    "\n",
    "# Verify formatting remains visually correct\n",
    "print(tokenizer.apply_chat_template(train_dataset[100][\"messages\"], tokenize=False, add_generation_prompt=False, return_tensors=\"pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489219f",
   "metadata": {},
   "source": [
    "## 3. LoRA Instruction-Tuning <a id=\"part_03\"></a>\n",
    "\n",
    "This section defines the LoRA configuration and training procedure for supervised fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e5c52",
   "metadata": {},
   "source": [
    "### 3.1 LoRA Configuration <a id=\"31-lora-configuration\"></a>\n",
    "\n",
    "The **Low-Rank Adaptation (LoRA)** parameters determine which layers are trained and how\n",
    "the adapter weights are integrated with the base model.\n",
    "\n",
    "| Parameter        | Value                               | Description                                      |\n",
    "| :--------------- | :---------------------------------- | :----------------------------------------------- |\n",
    "| `r`              | 32                                  | Low-rank dimension of trainable adapter matrices |\n",
    "| `alpha`          | 64                                  | Scaling factor (≈ 2× rank)                       |\n",
    "| `dropout`        | 0.05                                | Regularization term                              |\n",
    "| `target_modules` | q/k/v/o, gate, up, down projections | Core transformer attention modules               |\n",
    "\n",
    "These settings follow standard SFT practices for instruction tuning small-to-medium models\n",
    "and align with configurations used in recent studies (e.g., *LIMIT*, *Unveiling the Secret Recipe*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "996799f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Config for Instruction Tuning:\n",
      "  Rank: 32\n",
      "  Alpha: 64\n",
      "  Scaling factor: 2.0\n",
      "  Target modules: {'down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj'}\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,                           # Rank - using different rank than existing (which was 128)\n",
    "    lora_alpha=64,                  # Alpha = Scaling factor - usually 2x the rank\n",
    "    lora_dropout=0.05,              # Dropout - regularization for instruction tuning\n",
    "    bias=\"none\",                    # Whether to train bias parameters\n",
    "    task_type=\"CAUSAL_LM\",          # Task type for the model\n",
    "    target_modules=[                # Target modules for LoRA\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    ")\n",
    "print(\"LoRA Config for Instruction Tuning:\")\n",
    "print(f\"  Rank: {lora_config.r}\")\n",
    "print(f\"  Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  Scaling factor: {lora_config.lora_alpha / lora_config.r}\")  # Should be ~2\n",
    "print(f\"  Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa00daa",
   "metadata": {},
   "source": [
    "### 3.2 Training Setup <a id=\"32-training-setup\"></a>\n",
    "\n",
    "Fine-tuning is implemented with the **SFTTrainer** class from the TRL library.\n",
    "The following configuration controls optimization behavior, checkpointing, and memory usage.\n",
    "\n",
    "| Parameter                   | Value                    |\n",
    "| :-------------------------- | :----------------------- |\n",
    "| Epochs                      | 2.5                      |\n",
    "| Max Sequence Length         | 768 tokens               |\n",
    "| Effective Batch Size        | 16 (1 × 16 accumulation) |\n",
    "| Learning Rate               | 1e-4                     |\n",
    "| Scheduler                   | Cosine                   |\n",
    "| Warmup Ratio                | 0.1                      |\n",
    "| Gradient Clipping           | 1.0                      |\n",
    "| Precision                   | bfloat16                 |\n",
    "| Evaluation & Save Frequency | every 50 steps           |\n",
    "\n",
    "`assistant_only_loss=True` ensures the model learns **only from assistant responses**,\n",
    "preserving stable conversational patterns during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured:\n",
      "  Epochs: 2.5\n",
      "  Effective batch size: 16\n",
      "  Learning rate: 0.0001\n",
      "  Output dir: ../../models/gemma-3-1b-pt-contextual-e1-ckpt1600-sft115\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=str(OUTPUT_MODEL_DIR),\n",
    "\n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=2.5,               # Slightly more epochs for instruction tuning\n",
    "    max_length=768,                     # Maximum sequence length for the model inputs\n",
    "\n",
    "    # Data & Loss Handling\n",
    "    assistant_only_loss=True,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"messages\",\n",
    "\n",
    "    # Batch size 1 X 16 = 16 examples per step (adjust based on GPU memory)\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "\n",
    "    # Learning rate - slightly lower for fine-tuning on top of existing adapters\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    # Optimization\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Evaluation and logging\n",
    "    save_total_limit=4,\n",
    "    load_best_model_at_end=False,        # Causes issues with multi-adapter\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=25,\n",
    "    \n",
    "    # Performance\n",
    "    bf16=torch.cuda.is_available(),     # Use bfloat16 if available\n",
    "    dataloader_num_workers=0,           # Dataloader safety (Windows/WSL sometimes has problem with workers)\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={     # Critical for LoRA stability\n",
    "        'use_reentrant':False\n",
    "    },\n",
    "\n",
    "    # Other\n",
    "    report_to=\"none\",\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(\n",
    "    f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Output dir: {OUTPUT_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "368349d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeneto/projects/msc-2s2025_SSRL/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9011d033d66a47e2a7fb281b54bb0c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/2177 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988dedc091e04d7d8f2e96dd9a41580e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/2177 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0737f3bee243378a595d4d0593d211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161e963fff7a480fad16360bd654884d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized. Ready to start training!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=lora_config,        # LoRA configuration\n",
    "    processing_class=tokenizer      # Argument for tokenizer\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f3a300",
   "metadata": {},
   "source": [
    "### 3.3 Sanity Checks <a id=\"33-sanity-checks\"></a>\n",
    "\n",
    "Before training begins, two diagnostic steps validate dataset integrity and loss computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b0079",
   "metadata": {},
   "source": [
    "This cell performs a **sanity check** on the training dataset to ensure the model inputs are formatted correctly.\n",
    "\n",
    "Given a sample, It **decodes input IDs** and converts numerical tokens back into text using the tokenizer.  \n",
    "This reveals the **actual prompt structure** the model sees, including special control tokens (e.g., `<start_of_turn>`, `<bos>`) and the formatting of the Q&A pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc49b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WHAT THE MODEL SEES ---\n",
      "<bos><start_of_turn>user\n",
      "Você é um assistente especialista que responde questões de múltipla escolha em português do Brasil.\n",
      "Responda apenas com UMA opção correta (A, B, C ou D).\n",
      "Assunto: Business Ethics\n",
      "\n",
      "Pergunta: Os gerentes têm a confiança de administrar a empresa aos melhores interesses dos _______. Especificamente, eles têm o dever de agir ao benefício da empresa, além do dever de __________ e de ____________.\n",
      "A) Acionistas, Cuidado e habilidade, Diligência.\n",
      "B) Partes interessadas, cuidado e habilidade, Diligência\n",
      "C) Acionistas, Interesse próprio, Diligência\n",
      "D) Acionistas, Cuidado e habilidade, Interesse próprio\n",
      "\n",
      "Resposta correta:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "A) Acionistas, Cuidado e habilidade, Diligência.<end_of_turn>\n",
      "\n",
      "---------------------------\n",
      "Token count: 180\n"
     ]
    }
   ],
   "source": [
    "# Verify the trainer processed the dataset\n",
    "processed_sample = trainer.train_dataset[100]\n",
    "\n",
    "# Decode the Input IDs back to text to see what the model actually sees\n",
    "decoded_text = tokenizer.decode(processed_sample['input_ids'])\n",
    "\n",
    "print(\"--- WHAT THE MODEL SEES ---\")\n",
    "print(decoded_text)\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Check Length\n",
    "print(f\"Token count: {len(processed_sample['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec86d02",
   "metadata": {},
   "source": [
    "This cell checks **how tokens and labels are prepared** by the data collator before training.  \n",
    "It prints each token’s ID and verifies which ones are masked (`label = -100`) — meaning they don’t contribute to the loss.\n",
    "\n",
    "A **good result** shows that only the **assistant’s response tokens** are labeled (non -100), and everything else (user prompt, padding) is ignored.\n",
    "\n",
    "A **bad result** — like here — shows that the special token `<end_of_turn>` is masked (`-100`), meaning the model **never learns when to stop** its answer.  \n",
    "That can lead to repeated or runaway outputs during inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8846a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked tokens: tensor(11)\n",
      "Unmasked tokens: tensor(302)\n",
      "--> THIS Token is IGNORED '<bos>' | Label ID: -100\n",
      "--> THIS Token is IGNORED '<start_of_turn>' | Label ID: -100\n",
      "--> THIS Token is IGNORED 'user' | Label ID: -100\n",
      "--> THIS Token is IGNORED '\n",
      "' | Label ID: -100\n",
      "Token at 4: 'Você' | Label ID: 88270\n",
      "Token at 5: ' é' | Label ID: 1559\n",
      "Token at 6: ' um' | Label ID: 1983\n",
      "Token at 7: ' assist' | Label ID: 6361\n",
      "Token at 8: 'ente' | Label ID: 3194\n",
      "Token at 9: ' especialista' | Label ID: 127835\n",
      "Token at 10: ' que' | Label ID: 929\n",
      "Token at 11: ' responde' | Label ID: 106451\n",
      "Token at 12: ' questões' | Label ID: 82829\n",
      "Token at 13: ' de' | Label ID: 569\n",
      "Token at 14: ' múlti' | Label ID: 80642\n",
      "Token at 15: 'pla' | Label ID: 30635\n",
      "Token at 16: ' escolha' | Label ID: 97358\n",
      "Token at 17: ' em' | Label ID: 1092\n",
      "Token at 18: ' português' | Label ID: 130383\n",
      "Token at 19: ' do' | Label ID: 776\n",
      "Token at 20: ' Brasil' | Label ID: 23463\n",
      "Token at 21: '.' | Label ID: 236761\n",
      "Token at 22: '\n",
      "' | Label ID: 107\n",
      "Token at 23: 'Res' | Label ID: 1988\n",
      "Token at 24: 'pon' | Label ID: 966\n",
      "Token at 25: 'da' | Label ID: 1926\n",
      "Token at 26: ' apenas' | Label ID: 28091\n",
      "Token at 27: ' com' | Label ID: 631\n",
      "Token at 28: ' U' | Label ID: 749\n",
      "Token at 29: 'MA' | Label ID: 4868\n",
      "Token at 30: ' opção' | Label ID: 96813\n",
      "Token at 31: ' corre' | Label ID: 11289\n",
      "Token at 32: 'ta' | Label ID: 1246\n",
      "Token at 33: ' (' | Label ID: 568\n",
      "Token at 34: 'A' | Label ID: 236776\n",
      "Token at 35: ',' | Label ID: 236764\n",
      "Token at 36: ' B' | Label ID: 603\n",
      "Token at 37: ',' | Label ID: 236764\n",
      "Token at 38: ' C' | Label ID: 565\n",
      "Token at 39: ' ou' | Label ID: 3349\n",
      "Token at 40: ' D' | Label ID: 622\n",
      "Token at 41: ').' | Label ID: 769\n",
      "Token at 42: '\n",
      "' | Label ID: 107\n",
      "Token at 43: 'Ass' | Label ID: 7721\n",
      "Token at 44: 'unto' | Label ID: 16212\n",
      "Token at 45: ':' | Label ID: 236787\n",
      "Token at 46: ' Professional' | Label ID: 23471\n",
      "Token at 47: ' Law' | Label ID: 6563\n",
      "Token at 48: '\n",
      "\n",
      "' | Label ID: 108\n",
      "Token at 49: 'Pergunta' | Label ID: 154049\n",
      "Token at 50: ':' | Label ID: 236787\n",
      "Token at 51: ' O' | Label ID: 708\n",
      "Token at 52: ' prédio' | Label ID: 156321\n",
      "Token at 53: ' de' | Label ID: 569\n",
      "Token at 54: ' um' | Label ID: 1983\n",
      "Token at 55: ' restaurante' | Label ID: 84179\n",
      "Token at 56: ' foi' | Label ID: 11902\n",
      "Token at 57: ' ampl' | Label ID: 10870\n",
      "Token at 58: 'iado' | Label ID: 83770\n",
      "Token at 59: ' para' | Label ID: 1642\n",
      "Token at 60: ' um' | Label ID: 1983\n",
      "Token at 61: ' terreno' | Label ID: 69475\n",
      "Token at 62: ' bald' | Label ID: 49636\n",
      "Token at 63: 'io' | Label ID: 897\n",
      "Token at 64: ' ao' | Label ID: 8753\n",
      "Token at 65: ' norte' | Label ID: 48132\n",
      "Token at 66: ' da' | Label ID: 1776\n",
      "Token at 67: ' sua' | Label ID: 9563\n",
      "Token at 68: ' localização' | Label ID: 144371\n",
      "Token at 69: ' original' | Label ID: 3303\n",
      "Token at 70: '.' | Label ID: 236761\n",
      "Token at 71: ' O' | Label ID: 708\n",
      "Token at 72: ' empre' | Label ID: 50312\n",
      "Token at 73: 'ite' | Label ID: 785\n",
      "Token at 74: 'iro' | Label ID: 6175\n",
      "Token at 75: ' que' | Label ID: 929\n",
      "Token at 76: ' estava' | Label ID: 44627\n",
      "Token at 77: ' trabalhando' | Label ID: 139720\n",
      "Token at 78: ' na' | Label ID: 1784\n",
      "Token at 79: ' expans' | Label ID: 34312\n",
      "Token at 80: 'ão' | Label ID: 1592\n",
      "Token at 81: ' do' | Label ID: 776\n",
      "Token at 82: ' restaurante' | Label ID: 84179\n",
      "Token at 83: ' descob' | Label ID: 96980\n",
      "Token at 84: 'riu' | Label ID: 99945\n",
      "Token at 85: ' que' | Label ID: 929\n",
      "Token at 86: ' a' | Label ID: 496\n",
      "Token at 87: ' parede' | Label ID: 145643\n",
      "Token at 88: ' norte' | Label ID: 48132\n",
      "Token at 89: ' da' | Label ID: 1776\n",
      "Token at 90: ' nova' | Label ID: 38276\n",
      "Token at 91: ' estrutura' | Label ID: 87650\n",
      "Token at 92: ' precis' | Label ID: 13980\n",
      "Token at 93: 'ava' | Label ID: 1797\n",
      "Token at 94: ' de' | Label ID: 569\n",
      "Token at 95: ' amplo' | Label ID: 222964\n",
      "Token at 96: ' suporte' | Label ID: 130095\n",
      "Token at 97: ',' | Label ID: 236764\n",
      "Token at 98: ' então' | Label ID: 26797\n",
      "Token at 99: ' foram' | Label ID: 32410\n",
      "Token at 100: ' acrescent' | Label ID: 180790\n",
      "Token at 101: 'adas' | Label ID: 5102\n",
      "Token at 102: ' hast' | Label ID: 23823\n",
      "Token at 103: 'es' | Label ID: 507\n",
      "Token at 104: ' de' | Label ID: 569\n",
      "Token at 105: ' ancor' | Label ID: 221721\n",
      "Token at 106: 'agem' | Label ID: 13487\n",
      "Token at 107: ' e' | Label ID: 545\n",
      "Token at 108: ' concreto' | Label ID: 79009\n",
      "Token at 109: '.' | Label ID: 236761\n",
      "Token at 110: ' As' | Label ID: 1773\n",
      "Token at 111: ' hast' | Label ID: 23823\n",
      "Token at 112: 'es' | Label ID: 507\n",
      "Token at 113: ' de' | Label ID: 569\n",
      "Token at 114: ' ancor' | Label ID: 221721\n",
      "Token at 115: 'agem' | Label ID: 13487\n",
      "Token at 116: ' e' | Label ID: 545\n",
      "Token at 117: ' o' | Label ID: 512\n",
      "Token at 118: ' concreto' | Label ID: 79009\n",
      "Token at 119: ' se' | Label ID: 636\n",
      "Token at 120: ' est' | Label ID: 1009\n",
      "Token at 121: 'end' | Label ID: 643\n",
      "Token at 122: 'iam' | Label ID: 3488\n",
      "Token at 123: ' até' | Label ID: 19431\n",
      "Token at 124: ' uma' | Label ID: 4694\n",
      "Token at 125: ' propriedade' | Label ID: 137343\n",
      "Token at 126: ' viz' | Label ID: 29765\n",
      "Token at 127: 'inha' | Label ID: 30307\n",
      "Token at 128: ',' | Label ID: 236764\n",
      "Token at 129: ' mais' | Label ID: 4522\n",
      "Token at 130: ' ao' | Label ID: 8753\n",
      "Token at 131: ' norte' | Label ID: 48132\n",
      "Token at 132: ',' | Label ID: 236764\n",
      "Token at 133: ' ' | Label ID: 236743\n",
      "Token at 134: '2' | Label ID: 236778\n",
      "Token at 135: '0' | Label ID: 236771\n",
      "Token at 136: ' pés' | Label ID: 162843\n",
      "Token at 137: ' abaixo' | Label ID: 74424\n",
      "Token at 138: ' da' | Label ID: 1776\n",
      "Token at 139: ' superfície' | Label ID: 144721\n",
      "Token at 140: '.' | Label ID: 236761\n",
      "Token at 141: ' Embora' | Label ID: 190396\n",
      "Token at 142: ' não' | Label ID: 6118\n",
      "Token at 143: ' tenha' | Label ID: 71162\n",
      "Token at 144: ' hav' | Label ID: 43103\n",
      "Token at 145: 'ido' | Label ID: 2713\n",
      "Token at 146: ' impacto' | Label ID: 54660\n",
      "Token at 147: ' na' | Label ID: 1784\n",
      "Token at 148: ' superfície' | Label ID: 144721\n",
      "Token at 149: ' do' | Label ID: 776\n",
      "Token at 150: ' seu' | Label ID: 9741\n",
      "Token at 151: ' terreno' | Label ID: 69475\n",
      "Token at 152: ' ou' | Label ID: 3349\n",
      "Token at 153: ' nos' | Label ID: 4922\n",
      "Token at 154: ' usos' | Label ID: 157113\n",
      "Token at 155: ' existentes' | Label ID: 113142\n",
      "Token at 156: ',' | Label ID: 236764\n",
      "Token at 157: ' o' | Label ID: 512\n",
      "Token at 158: ' propriet' | Label ID: 43073\n",
      "Token at 159: 'ário' | Label ID: 13781\n",
      "Token at 160: ' do' | Label ID: 776\n",
      "Token at 161: ' imóvel' | Label ID: 167763\n",
      "Token at 162: ' viz' | Label ID: 29765\n",
      "Token at 163: 'inho' | Label ID: 19777\n",
      "Token at 164: ' process' | Label ID: 1657\n",
      "Token at 165: 'ou' | Label ID: 521\n",
      "Token at 166: ' o' | Label ID: 512\n",
      "Token at 167: ' dono' | Label ID: 120909\n",
      "Token at 168: ' do' | Label ID: 776\n",
      "Token at 169: ' restaurante' | Label ID: 84179\n",
      "Token at 170: ' por' | Label ID: 1839\n",
      "Token at 171: ' invas' | Label ID: 32788\n",
      "Token at 172: 'ão' | Label ID: 1592\n",
      "Token at 173: ' de' | Label ID: 569\n",
      "Token at 174: ' propriedade' | Label ID: 137343\n",
      "Token at 175: '.' | Label ID: 236761\n",
      "Token at 176: ' Qual' | Label ID: 19619\n",
      "Token at 177: ' parte' | Label ID: 7191\n",
      "Token at 178: ' tem' | Label ID: 1481\n",
      "Token at 179: ' mais' | Label ID: 4522\n",
      "Token at 180: ' chances' | Label ID: 18190\n",
      "Token at 181: ' de' | Label ID: 569\n",
      "Token at 182: ' sair' | Label ID: 95365\n",
      "Token at 183: ' venced' | Label ID: 158178\n",
      "Token at 184: 'ora' | Label ID: 3509\n",
      "Token at 185: '?' | Label ID: 236881\n",
      "Token at 186: '\n",
      "' | Label ID: 107\n",
      "Token at 187: 'A' | Label ID: 236776\n",
      "Token at 188: ')' | Label ID: 236768\n",
      "Token at 189: ' O' | Label ID: 708\n",
      "Token at 190: ' propriet' | Label ID: 43073\n",
      "Token at 191: 'ário' | Label ID: 13781\n",
      "Token at 192: ' do' | Label ID: 776\n",
      "Token at 193: ' imóvel' | Label ID: 167763\n",
      "Token at 194: ' viz' | Label ID: 29765\n",
      "Token at 195: 'inho' | Label ID: 19777\n",
      "Token at 196: ',' | Label ID: 236764\n",
      "Token at 197: ' pois' | Label ID: 41396\n",
      "Token at 198: ' deveria' | Label ID: 165627\n",
      "Token at 199: ' ter' | Label ID: 2814\n",
      "Token at 200: ' sido' | Label ID: 18150\n",
      "Token at 201: ' avis' | Label ID: 53805\n",
      "Token at 202: 'ado' | Label ID: 1438\n",
      "Token at 203: ' da' | Label ID: 1776\n",
      "Token at 204: ' intr' | Label ID: 10646\n",
      "Token at 205: 'us' | Label ID: 605\n",
      "Token at 206: 'ão' | Label ID: 1592\n",
      "Token at 207: ' antes' | Label ID: 15349\n",
      "Token at 208: ' que' | Label ID: 929\n",
      "Token at 209: ' ocor' | Label ID: 58656\n",
      "Token at 210: 'resse' | Label ID: 24946\n",
      "Token at 211: '.' | Label ID: 236761\n",
      "Token at 212: '\n",
      "' | Label ID: 107\n",
      "Token at 213: 'B' | Label ID: 236799\n",
      "Token at 214: ')' | Label ID: 236768\n",
      "Token at 215: ' O' | Label ID: 708\n",
      "Token at 216: ' propriet' | Label ID: 43073\n",
      "Token at 217: 'ário' | Label ID: 13781\n",
      "Token at 218: ' do' | Label ID: 776\n",
      "Token at 219: ' imóvel' | Label ID: 167763\n",
      "Token at 220: ' viz' | Label ID: 29765\n",
      "Token at 221: 'inho' | Label ID: 19777\n",
      "Token at 222: ',' | Label ID: 236764\n",
      "Token at 223: ' pois' | Label ID: 41396\n",
      "Token at 224: ' o' | Label ID: 512\n",
      "Token at 225: ' restaurante' | Label ID: 84179\n",
      "Token at 226: ' inv' | Label ID: 1379\n",
      "Token at 227: 'adi' | Label ID: 2175\n",
      "Token at 228: 'u' | Label ID: 236756\n",
      "Token at 229: ' sua' | Label ID: 9563\n",
      "Token at 230: ' propriedade' | Label ID: 137343\n",
      "Token at 231: ' sem' | Label ID: 2888\n",
      "Token at 232: ' permiss' | Label ID: 209477\n",
      "Token at 233: 'ão' | Label ID: 1592\n",
      "Token at 234: '.' | Label ID: 236761\n",
      "Token at 235: '\n",
      "' | Label ID: 107\n",
      "Token at 236: 'C' | Label ID: 236780\n",
      "Token at 237: ')' | Label ID: 236768\n",
      "Token at 238: ' O' | Label ID: 708\n",
      "Token at 239: ' dono' | Label ID: 120909\n",
      "Token at 240: ' do' | Label ID: 776\n",
      "Token at 241: ' restaurante' | Label ID: 84179\n",
      "Token at 242: ',' | Label ID: 236764\n",
      "Token at 243: ' pois' | Label ID: 41396\n",
      "Token at 244: ' a' | Label ID: 496\n",
      "Token at 245: ' decisão' | Label ID: 107604\n",
      "Token at 246: ' de' | Label ID: 569\n",
      "Token at 247: ' colocar' | Label ID: 50582\n",
      "Token at 248: ' o' | Label ID: 512\n",
      "Token at 249: ' apoio' | Label ID: 85999\n",
      "Token at 250: ' adicional' | Label ID: 56562\n",
      "Token at 251: ' foi' | Label ID: 11902\n",
      "Token at 252: ' sens' | Label ID: 3613\n",
      "Token at 253: 'ata' | Label ID: 805\n",
      "Token at 254: '.' | Label ID: 236761\n",
      "Token at 255: '\n",
      "' | Label ID: 107\n",
      "Token at 256: 'D' | Label ID: 236796\n",
      "Token at 257: ')' | Label ID: 236768\n",
      "Token at 258: ' O' | Label ID: 708\n",
      "Token at 259: ' dono' | Label ID: 120909\n",
      "Token at 260: ' do' | Label ID: 776\n",
      "Token at 261: ' restaurante' | Label ID: 84179\n",
      "Token at 262: ',' | Label ID: 236764\n",
      "Token at 263: ' pois' | Label ID: 41396\n",
      "Token at 264: ' não' | Label ID: 6118\n",
      "Token at 265: ' houve' | Label ID: 148146\n",
      "Token at 266: ' pert' | Label ID: 9988\n",
      "Token at 267: 'ur' | Label ID: 556\n",
      "Token at 268: 'ba' | Label ID: 3604\n",
      "Token at 269: 'ção' | Label ID: 2758\n",
      "Token at 270: ' da' | Label ID: 1776\n",
      "Token at 271: ' fru' | Label ID: 42844\n",
      "Token at 272: 'ição' | Label ID: 20965\n",
      "Token at 273: ' tranquila' | Label ID: 195546\n",
      "Token at 274: ' do' | Label ID: 776\n",
      "Token at 275: ' imóvel' | Label ID: 167763\n",
      "Token at 276: ' viz' | Label ID: 29765\n",
      "Token at 277: 'inho' | Label ID: 19777\n",
      "Token at 278: '.' | Label ID: 236761\n",
      "Token at 279: '\n",
      "\n",
      "' | Label ID: 108\n",
      "Token at 280: 'Resposta' | Label ID: 105494\n",
      "Token at 281: ' corre' | Label ID: 11289\n",
      "Token at 282: 'ta' | Label ID: 1246\n",
      "Token at 283: ':' | Label ID: 236787\n",
      "--> THIS Token is IGNORED '<end_of_turn>' | Label ID: -100\n",
      "--> THIS Token is IGNORED '\n",
      "' | Label ID: -100\n",
      "--> THIS Token is IGNORED '<start_of_turn>' | Label ID: -100\n",
      "--> THIS Token is IGNORED 'model' | Label ID: -100\n",
      "--> THIS Token is IGNORED '\n",
      "' | Label ID: -100\n",
      "Token at 289: 'B' | Label ID: 236799\n",
      "Token at 290: ')' | Label ID: 236768\n",
      "Token at 291: ' O' | Label ID: 708\n",
      "Token at 292: ' propriet' | Label ID: 43073\n",
      "Token at 293: 'ário' | Label ID: 13781\n",
      "Token at 294: ' do' | Label ID: 776\n",
      "Token at 295: ' imóvel' | Label ID: 167763\n",
      "Token at 296: ' viz' | Label ID: 29765\n",
      "Token at 297: 'inho' | Label ID: 19777\n",
      "Token at 298: ',' | Label ID: 236764\n",
      "Token at 299: ' pois' | Label ID: 41396\n",
      "Token at 300: ' o' | Label ID: 512\n",
      "Token at 301: ' restaurante' | Label ID: 84179\n",
      "Token at 302: ' inv' | Label ID: 1379\n",
      "Token at 303: 'adi' | Label ID: 2175\n",
      "Token at 304: 'u' | Label ID: 236756\n",
      "Token at 305: ' sua' | Label ID: 9563\n",
      "Token at 306: ' propriedade' | Label ID: 137343\n",
      "Token at 307: ' sem' | Label ID: 2888\n",
      "Token at 308: ' permiss' | Label ID: 209477\n",
      "Token at 309: 'ão' | Label ID: 1592\n",
      "Token at 310: '.' | Label ID: 236761\n",
      "--> THIS Token is IGNORED '<end_of_turn>' | Label ID: -100\n",
      "--> THIS Token is IGNORED '\n",
      "' | Label ID: -100\n"
     ]
    }
   ],
   "source": [
    "# Inspecting Token Labels and Masking Behavior\n",
    "sample = trainer.train_dataset[0]   # Get one sample\n",
    "\n",
    "# Use the Data Collator (which applies the masking)\n",
    "collator = trainer.data_collator\n",
    "batch = collator([sample])\n",
    "\n",
    "# Extract Input IDs and Labels\n",
    "input_ids = batch['input_ids'][0]\n",
    "labels = batch['labels'][0]\n",
    "\n",
    "print(\"Masked tokens:\", sum(l==-100 for l in batch['labels'][0]))\n",
    "print(\"Unmasked tokens:\", sum(l!=-100 for l in batch['labels'][0]))\n",
    "\n",
    "# Find where the answer \"A/B/C/D\" is\n",
    "for i, label_id in enumerate(labels):\n",
    "    # Decode the actual input token for inspection\n",
    "    input_token_str = tokenizer.decode([input_ids[i].item()])\n",
    "    if label_id != -100:\n",
    "        decoded_token = tokenizer.decode([label_id])\n",
    "        decoded_label = labels[i].item()\n",
    "        decoded_next_label = labels[i+1].item() if i+1 < len(labels) else \"End\"\n",
    "        \n",
    "        print(f\"Token at {i}: '{decoded_token}' | Label ID: {decoded_label}\")\n",
    "    else:\n",
    "        # If label IS -100, the token is IGNORED in loss calculation\n",
    "        decoded_next_label = labels[i+1].item() if i+1 < len(labels) else \"End\"\n",
    "        print(f\"--> THIS Token is IGNORED '{input_token_str}' | Label ID: {label_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e2b05",
   "metadata": {},
   "source": [
    "### 3.4 Training <a id=\"34-training\"></a>\n",
    "\n",
    "The model is fine-tuned using LoRA adapters with the configured SFTTrainer.\n",
    "Each step logs loss, token accuracy, and gradient statistics.\n",
    "\n",
    "**Logged Metrics:**\n",
    "\n",
    "* `step` — logging step index within the epoch\n",
    "* `training_loss` — cross-entropy loss on the training split (unmasked tokens)\n",
    "* `validation_loss` — cross-entropy loss on the validation split\n",
    "* `entropy` — average uncertainty of model outputs\n",
    "* `num_tokens` — total number of tokens processed per logging step\n",
    "* `mean_token_accuracy` — token-level prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5812f7c7",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning:** The next Python cell involves **high-performance computing (HPC)**.  \n",
    "> Execution requires a **dedicated or cloud machine with multiple cores**, not a standard desktop or notebook.  \n",
    "> Runtime and cell outputs are reported below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6455252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='343' max='343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [343/343 1:18:18, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.407400</td>\n",
       "      <td>1.220184</td>\n",
       "      <td>1.216521</td>\n",
       "      <td>220148.000000</td>\n",
       "      <td>0.743835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.325100</td>\n",
       "      <td>1.174997</td>\n",
       "      <td>1.203234</td>\n",
       "      <td>434520.000000</td>\n",
       "      <td>0.752394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.245000</td>\n",
       "      <td>1.166622</td>\n",
       "      <td>1.105936</td>\n",
       "      <td>645252.000000</td>\n",
       "      <td>0.755691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.222200</td>\n",
       "      <td>1.159029</td>\n",
       "      <td>1.118751</td>\n",
       "      <td>858136.000000</td>\n",
       "      <td>0.756214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.205800</td>\n",
       "      <td>1.155876</td>\n",
       "      <td>1.086265</td>\n",
       "      <td>1074267.000000</td>\n",
       "      <td>0.757525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.135600</td>\n",
       "      <td>1.165259</td>\n",
       "      <td>1.040830</td>\n",
       "      <td>1289655.000000</td>\n",
       "      <td>0.756023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=343, training_loss=1.303121566772461, metrics={'train_runtime': 4712.0263, 'train_samples_per_second': 1.155, 'train_steps_per_second': 0.073, 'total_flos': 6406230200577024.0, 'train_loss': 1.303121566772461, 'entropy': 1.009279995639291, 'num_tokens': 1474756.0, 'mean_token_accuracy': 0.7776619887186421, 'epoch': 2.5071198897565456})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac05d7e",
   "metadata": {},
   "source": [
    "After training completes, the best checkpoint is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b712e634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LoRA adapters saved to: ../../models/gemma-3-1b-pt-contextual-e1-ckpt1600-sft115/best_eval\n",
      "Tokenizer saved to: ../../models/gemma-3-1b-pt-contextual-e1-ckpt1600-sft115/best_eval\n"
     ]
    }
   ],
   "source": [
    "# Save the final LoRA adapters\n",
    "final_model_path = OUTPUT_MODEL_DIR / \"best_eval\"\n",
    "trainer.save_model(str(final_model_path))\n",
    "\n",
    "print(f\"Final LoRA adapters saved to: {final_model_path}\")\n",
    "\n",
    "# Also save the tokenizer to the same location\n",
    "tokenizer.save_pretrained(str(final_model_path))\n",
    "print(f\"Tokenizer saved to: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083471ea",
   "metadata": {},
   "source": [
    "**Final Outputs:**\n",
    "\n",
    "* `gemma-3-1b-pt-contextual-e1-ckpt1600-sft/best_eval/adapter_model.bin` — fine-tuned LoRA weights\n",
    "* `tokenizer.json` — tokenizer configuration for inference and evaluation\n",
    "* `trainer_state.json` — loss and accuracy progression logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-2s2025-ssrl-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
