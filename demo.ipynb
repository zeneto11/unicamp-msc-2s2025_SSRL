{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf78b0b",
   "metadata": {},
   "source": [
    "## 1. Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "113c8234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import platform\n",
    "import re\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset\n",
    "from peft import (LoraConfig, PeftModel, get_peft_model,\n",
    "                  prepare_model_for_kbit_training)\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          BitsAndBytesConfig, DataCollatorForLanguageModeling,\n",
    "                          TextStreamer, Trainer, TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0d8df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "CUDA version: 12.6\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available ‚Äî running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39a55d",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95bd1d",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "520decd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219674, 11)\n",
      "Index(['identifier', 'taxonID', 'type', 'format', 'CVterm', 'title',\n",
      "       'description', 'furtherInformationURL', 'language', 'UsageTerms',\n",
      "       'Owner'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>taxonID</th>\n",
       "      <th>type</th>\n",
       "      <th>format</th>\n",
       "      <th>CVterm</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>furtherInformationURL</th>\n",
       "      <th>language</th>\n",
       "      <th>UsageTerms</th>\n",
       "      <th>Owner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72fb7e4d377fc9601bac5f4e5f3b63a2</td>\n",
       "      <td>Q140</td>\n",
       "      <td>http://purl.org/dc/dcmitype/Text</td>\n",
       "      <td>text/html</td>\n",
       "      <td>http://rs.tdwg.org/ontology/voc/SPMInfoItems#D...</td>\n",
       "      <td>Le√£o</td>\n",
       "      <td>&gt;&lt;div lang=\"pt\" dir=\"ltr\"&gt; &lt;p&gt;O &lt;b&gt;le√£o&lt;/b&gt;&lt;su...</td>\n",
       "      <td>http://pt.wikipedia.org/w/index.php?title=Le%C...</td>\n",
       "      <td>pt</td>\n",
       "      <td>http://creativecommons.org/licenses/by-sa/3.0/</td>\n",
       "      <td>Autores e editores de Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facbaa84c032fa36235f7d9e8fa34192</td>\n",
       "      <td>Q140</td>\n",
       "      <td>http://purl.org/dc/dcmitype/Text</td>\n",
       "      <td>text/html</td>\n",
       "      <td>http://rs.tdwg.org/ontology/voc/SPMInfoItems#T...</td>\n",
       "      <td>Le√£o: Brief Summary</td>\n",
       "      <td>&gt; &lt;p&gt;O le√£o [feminino: leoa] (&lt;a href=\"http://...</td>\n",
       "      <td>http://pt.wikipedia.org/w/index.php?title=Le%C...</td>\n",
       "      <td>pt</td>\n",
       "      <td>http://creativecommons.org/licenses/by-sa/3.0/</td>\n",
       "      <td>Autores e editores de Wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52427a332e31b690daf46ef212302f1c</td>\n",
       "      <td>Q764</td>\n",
       "      <td>http://purl.org/dc/dcmitype/Text</td>\n",
       "      <td>text/html</td>\n",
       "      <td>http://rs.tdwg.org/ontology/voc/SPMInfoItems#D...</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>lang=\"pt\" dir=\"ltr\"&gt;&lt;div&gt; &lt;figure typeof=\"mw:F...</td>\n",
       "      <td>http://pt.wikipedia.org/w/index.php?title=Fung...</td>\n",
       "      <td>pt</td>\n",
       "      <td>http://creativecommons.org/licenses/by-sa/3.0/</td>\n",
       "      <td>Autores e editores de Wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         identifier taxonID                              type  \\\n",
       "0  72fb7e4d377fc9601bac5f4e5f3b63a2    Q140  http://purl.org/dc/dcmitype/Text   \n",
       "1  facbaa84c032fa36235f7d9e8fa34192    Q140  http://purl.org/dc/dcmitype/Text   \n",
       "2  52427a332e31b690daf46ef212302f1c    Q764  http://purl.org/dc/dcmitype/Text   \n",
       "\n",
       "      format                                             CVterm  \\\n",
       "0  text/html  http://rs.tdwg.org/ontology/voc/SPMInfoItems#D...   \n",
       "1  text/html  http://rs.tdwg.org/ontology/voc/SPMInfoItems#T...   \n",
       "2  text/html  http://rs.tdwg.org/ontology/voc/SPMInfoItems#D...   \n",
       "\n",
       "                 title                                        description  \\\n",
       "0                 Le√£o  ><div lang=\"pt\" dir=\"ltr\"> <p>O <b>le√£o</b><su...   \n",
       "1  Le√£o: Brief Summary  > <p>O le√£o [feminino: leoa] (<a href=\"http://...   \n",
       "2                Fungi  lang=\"pt\" dir=\"ltr\"><div> <figure typeof=\"mw:F...   \n",
       "\n",
       "                               furtherInformationURL language  \\\n",
       "0  http://pt.wikipedia.org/w/index.php?title=Le%C...       pt   \n",
       "1  http://pt.wikipedia.org/w/index.php?title=Le%C...       pt   \n",
       "2  http://pt.wikipedia.org/w/index.php?title=Fung...       pt   \n",
       "\n",
       "                                       UsageTerms  \\\n",
       "0  http://creativecommons.org/licenses/by-sa/3.0/   \n",
       "1  http://creativecommons.org/licenses/by-sa/3.0/   \n",
       "2  http://creativecommons.org/licenses/by-sa/3.0/   \n",
       "\n",
       "                             Owner  \n",
       "0  Autores e editores de Wikipedia  \n",
       "1  Autores e editores de Wikipedia  \n",
       "2  Autores e editores de Wikipedia  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the TSV file\n",
    "path = Path(\"data/zenodo_wikipedia-pt/media_resource.tab\")\n",
    "df = pd.read_csv(path, sep=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012131d",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0845be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean HTML from descriptions\n",
    "def clean_html_wikipedia(text):\n",
    "    \"\"\"Limpa HTML e res√≠duos textuais t√≠picos de dumps da Wikip√©dia\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    # Remove everything after 'Refer√™ncias' section\n",
    "    html = str(text)\n",
    "    html = re.split(r'id=\"Refer', html, flags=re.IGNORECASE)[0]\n",
    "\n",
    "    # Remove tags HTML\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "    # Remove references like [ 1 ], [ 23 ], [ note 5 ]\n",
    "    text = re.sub(r\"\\[\\s*\\d+\\s*\\]\", \"\", text)\n",
    "    # Remove patterns like 'editar c√≥digo-fonte'\n",
    "    text = re.sub(r\"editar c√≥digo[- ]fonte\", \"\", text, flags=re.IGNORECASE)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Remove spaces before punctuation\n",
    "    text = re.sub(r\"\\s([?.!,;:])\", r\"\\1\", text)\n",
    "    # Remove duplicated quotes and occasional unbalanced parentheses\n",
    "    text = text.replace(\"¬´ \", \"¬´\").replace(\" ¬ª\", \"¬ª\").strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "df[\"text\"] = df[\"description\"].apply(clean_html_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d62c24d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72fb7e4d377fc9601bac5f4e5f3b63a2</td>\n",
       "      <td>Le√£o</td>\n",
       "      <td>&gt; O le√£o [feminino: leoa ] ( nome cient√≠fico: ...</td>\n",
       "      <td>http://pt.wikipedia.org/w/index.php?title=Le%C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>facbaa84c032fa36235f7d9e8fa34192</td>\n",
       "      <td>Le√£o: Brief Summary</td>\n",
       "      <td>&gt; O le√£o [feminino: leoa] ( nome cient√≠fico: P...</td>\n",
       "      <td>http://pt.wikipedia.org/w/index.php?title=Le%C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52427a332e31b690daf46ef212302f1c</td>\n",
       "      <td>Fungi</td>\n",
       "      <td>lang=\"pt\" dir=\"ltr\"&gt; Estrutura√ß√£o filogen√©tica...</td>\n",
       "      <td>http://pt.wikipedia.org/w/index.php?title=Fung...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id                title  \\\n",
       "0  72fb7e4d377fc9601bac5f4e5f3b63a2                 Le√£o   \n",
       "1  facbaa84c032fa36235f7d9e8fa34192  Le√£o: Brief Summary   \n",
       "2  52427a332e31b690daf46ef212302f1c                Fungi   \n",
       "\n",
       "                                                text  \\\n",
       "0  > O le√£o [feminino: leoa ] ( nome cient√≠fico: ...   \n",
       "1  > O le√£o [feminino: leoa] ( nome cient√≠fico: P...   \n",
       "2  lang=\"pt\" dir=\"ltr\"> Estrutura√ß√£o filogen√©tica...   \n",
       "\n",
       "                                                 url  \n",
       "0  http://pt.wikipedia.org/w/index.php?title=Le%C...  \n",
       "1  http://pt.wikipedia.org/w/index.php?title=Le%C...  \n",
       "2  http://pt.wikipedia.org/w/index.php?title=Fung...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=[\"identifier\"])\n",
    "\n",
    "# Select relevant columns\n",
    "df_clean = df[[\n",
    "    \"identifier\", \"title\", \"text\", \"furtherInformationURL\"\n",
    "]]\n",
    "\n",
    "# Rename columns\n",
    "df_clean = df_clean.rename(columns={\n",
    "    \"identifier\": \"id\",\n",
    "    \"furtherInformationURL\": \"url\"\n",
    "})\n",
    "\n",
    "df_clean.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e94f01",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "725f4d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 219674/219674 [00:00<00:00, 1557412.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "dataset = Dataset.from_pandas(df_clean[[\"text\"]])\n",
    "\n",
    "# Save dataset\n",
    "dataset_path = Path(\"data/animal_wikipedia_pt_dataset\")\n",
    "dataset.save_to_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e3349",
   "metadata": {},
   "source": [
    "## 3. SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0732974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model\n",
    "model_name = \"./models/gemma-3-1b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3374aed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 9000\n",
      "Eval size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "dataset = Dataset.load_from_disk('data/animal_wikipedia_pt_dataset')\n",
    "\n",
    "# Sample dataset\n",
    "dataset_split = dataset.shuffle(seed=42).train_test_split(test_size=0.1)\n",
    "train_dataset = dataset_split['train'].select(range(min(9000, len(dataset_split['train']))))\n",
    "eval_dataset = dataset_split['test'].select(range(min(1000, len(dataset_split['test']))))\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Eval size: {len(eval_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0542d4",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "360d481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9000/9000 [00:00<00:00, 17405.11 examples/s]\n",
      "Tokenizing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 18189.76 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set size: 9000 examples\n",
      "Train set total tokens: 844,928\n",
      "Train set average tokens per example: 93.9\n",
      "\n",
      "Eval set size: 1000 examples\n",
      "Eval set total tokens: 90,494\n",
      "Eval set average tokens per example: 90.5\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Gemma needs this\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Add EOS token to help model learn when to stop\n",
    "    texts = [text + tokenizer.eos_token for text in examples[\"text\"]]\n",
    "    \n",
    "    result = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,  # Don't pad here, let collator handle it\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Tokenize dataset\n",
    "train_tokenized = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names, desc=\"Tokenizing\")\n",
    "eval_tokenized = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names, desc=\"Tokenizing\")\n",
    "\n",
    "# Count tokens for train and eval sets\n",
    "def count_tokens(tokenized_dataset):\n",
    "    return sum(len(input_ids) for input_ids in tokenized_dataset[\"input_ids\"])\n",
    "\n",
    "train_total_tokens = count_tokens(train_tokenized)\n",
    "train_avg_tokens = train_total_tokens / len(train_tokenized)\n",
    "\n",
    "eval_total_tokens = count_tokens(eval_tokenized)\n",
    "eval_avg_tokens = eval_total_tokens / len(eval_tokenized)\n",
    "\n",
    "print(f\"\\nTrain set size: {len(train_tokenized)} examples\")\n",
    "print(f\"Train set total tokens: {train_total_tokens:,}\")\n",
    "print(f\"Train set average tokens per example: {train_avg_tokens:.1f}\")\n",
    "\n",
    "print(f\"\\nEval set size: {len(eval_tokenized)} examples\")\n",
    "print(f\"Eval set total tokens: {eval_total_tokens:,}\")\n",
    "print(f\"Eval set average tokens per example: {eval_avg_tokens:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b2ef9",
   "metadata": {},
   "source": [
    "### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d6ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator for masked language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,              # Causal LM (GPT-style)\n",
    "    pad_to_multiple_of=8    # Optimization for GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8527c3cb",
   "metadata": {},
   "source": [
    "### Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb9a6f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared previous models from memory.\n",
      "Current allocated VRAM: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear any existing models from memory\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'trainer' in globals():\n",
    "    del trainer\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleared previous models from memory.\")\n",
    "print(f\"Current allocated VRAM: {torch.cuda.memory_allocated() / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fabfc33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "   - Device: cuda:0\n",
      "   - Dtype: torch.float16\n",
      "   - VRAM: 0.90GB\n"
     ]
    }
   ],
   "source": [
    "# Define 4-bit quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",           \n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Model loaded!\")\n",
    "print(f\"   - Device: {model.device}\")\n",
    "print(f\"   - Dtype: {model.dtype}\")\n",
    "print(f\"   - VRAM: {torch.cuda.memory_allocated(0) / (1024**3):.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "970aa436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient setup done: torch.float16 | VRAM: 0.90GB\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(f\"Gradient setup done: {model.dtype} | VRAM: {torch.cuda.memory_allocated(0) / (1024**3):.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19508d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,045,760 || all params: 1,012,931,712 || trainable%: 1.2879\n",
      "\n",
      "LoRA applied: torch.float16 | VRAM: 0.95GB\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,   \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(f\"\\nLoRA applied: {model.dtype} | VRAM: {torch.cuda.memory_allocated(0) / (1024**3):.2f}GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd61db",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6345bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments and trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{model_name.split(\"/\")[-1]}-animals-lora\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,   # Effective batch size = 4*4=16\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,              # Higher LR works well with LoRA\n",
    "    warmup_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",              \n",
    "    eval_steps=250,                     \n",
    "    per_device_eval_batch_size=8, \n",
    "    \n",
    "    # Logging & Saving\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,              # Keep only 2 checkpoints\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,     # Load best checkpoint at end\n",
    "    metric_for_best_model=\"loss\",    # Use loss to evaluate best model\n",
    "    \n",
    "    # Performance\n",
    "    fp16=True,                       # Mixed precision training\n",
    "    optim=\"paged_adamw_8bit\",        # Memory-efficient optimizer\n",
    "    gradient_checkpointing=True,     # Saves memory\n",
    "    \n",
    "    # Other\n",
    "    report_to=\"none\",                # or \"tensorboard\"/\"wandb\" if you want\n",
    "    dataloader_pin_memory=True,\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "492cfd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üíª System & Environment Check ---\n",
      "OS: Linux 6.6.87.2-microsoft-standard-WSL2\n",
      "Python: 3.12.3\n",
      "PyTorch CUDA: ‚úÖ Enabled (Device: NVIDIA GeForce RTX 4070 Laptop GPU)\n",
      "CUDA Version: 12.6\n",
      "Total CUDA VRAM: 8.00 GB\n",
      "\n",
      "--- üß† Model Loading & Verification ---\n",
      "Model loaded on: cuda:0\n",
      "Model dtype: torch.float16\n",
      "‚úÖ Model works! Ready to train.\n",
      "\n",
      "--- üìä Memory Usage Report ---\n",
      "GPU VRAM (Model + System): 1791MB / 8188MB (Free: 6397MB)\n",
      "PyTorch VRAM Allocated: 0.96 GB\n",
      "PyTorch VRAM Cached: 1.59 GB\n",
      "PyTorch Max VRAM Used: 1.12 GB (Since beginning)\n",
      "System RAM (Used): 8.22 GB / 15.37 GB\n",
      "\n",
      "--- ‚úçÔ∏è Text Generation Test ---\n",
      "Input: A baleia azul √©\n",
      "Generated: A baleia azul √© uma das maiores e mais belas criaturas do oceano.\n",
      "\n",
      "**Caracter√≠sticas:**\n",
      "\n",
      "*   **Tamanho:** As baleias azuis podem variar de 1,5 a 3 metros de comprimento.\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "# --- 1. System/GPU Info Functions ---\n",
    "def get_gpu_memory():\n",
    "    # Keep the original function to get Used/Total VRAM via nvidia-smi\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used,memory.total', \n",
    "             '--format=csv,nounits,noheader'], \n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        used, total = map(int, result.stdout.strip().split(','))\n",
    "        return used, total\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"‚ö†Ô∏è Warning: 'nvidia-smi' command failed. Are you on an NVIDIA GPU machine?\")\n",
    "        return 0, 0\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è Warning: 'nvidia-smi' not found. Ensure NVIDIA drivers are installed.\")\n",
    "        return 0, 0\n",
    "\n",
    "def get_cpu_memory():\n",
    "    # Get total system memory (RAM) and usage\n",
    "    ram = psutil.virtual_memory()\n",
    "    # Convert bytes to GB and return\n",
    "    return ram.total / (1024**3), ram.used / (1024**3)\n",
    "\n",
    "# --- 2. Print Initial Configuration ---\n",
    "print(\"--- üíª System & Environment Check ---\")\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "\n",
    "# CUDA/PyTorch checks\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch CUDA: ‚úÖ Enabled (Device: {torch.cuda.get_device_name(0)})\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    cuda_total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"Total CUDA VRAM: {cuda_total_memory:.2f} GB\")\n",
    "else:\n",
    "    print(\"PyTorch CUDA: ‚ùå Not Enabled. Check installation/drivers.\")\n",
    "\n",
    "# --- 3. Model Loading & Verification (Your original code) ---\n",
    "print(\"\\n--- üß† Model Loading & Verification ---\")\n",
    "print(f\"Model loaded on: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "\n",
    "# Try one forward pass\n",
    "sample = tokenizer(\"O le√£o √© um animal\", return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    output = model(**sample)\n",
    "print(\"‚úÖ Model works! Ready to train.\")\n",
    "\n",
    "# --- 4. Detailed Memory Check ---\n",
    "print(\"\\n--- üìä Memory Usage Report ---\")\n",
    "\n",
    "# GPU VRAM usage (from nvidia-smi)\n",
    "gpu_used_smi, gpu_total_smi = get_gpu_memory()\n",
    "gpu_free = gpu_total_smi - gpu_used_smi\n",
    "\n",
    "print(f\"GPU VRAM (Model + System): {gpu_used_smi}MB / {gpu_total_smi}MB (Free: {gpu_free}MB)\")\n",
    "\n",
    "# PyTorch Allocated/Cached Memory (More accurate for the current process)\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "    cached = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "    print(f\"PyTorch VRAM Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"PyTorch VRAM Cached: {cached:.2f} GB\")\n",
    "    print(f\"PyTorch Max VRAM Used: {torch.cuda.max_memory_allocated(0) / (1024**3):.2f} GB (Since beginning)\")\n",
    "\n",
    "# CPU RAM usage\n",
    "ram_total_gb, ram_used_gb = get_cpu_memory()\n",
    "print(f\"System RAM (Used): {ram_used_gb:.2f} GB / {ram_total_gb:.2f} GB\")\n",
    "\n",
    "# --- 5. Text Generation Example (Your original code) ---\n",
    "print(\"\\n--- ‚úçÔ∏è Text Generation Test ---\")\n",
    "input_text = \"A baleia azul √©\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=50,\n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a211ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 4:28:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.848000</td>\n",
       "      <td>1.841163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.826600</td>\n",
       "      <td>1.705029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.613900</td>\n",
       "      <td>1.660208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.571100</td>\n",
       "      <td>1.619913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.386300</td>\n",
       "      <td>1.634028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.434900</td>\n",
       "      <td>1.624888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1689, training_loss=1.671677661125585, metrics={'train_runtime': 16081.569, 'train_samples_per_second': 1.679, 'train_steps_per_second': 0.105, 'total_flos': 2.393596658029363e+16, 'train_loss': 1.671677661125585, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Trainer and start training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized, \n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba734e0",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00af445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Base Model Path\n",
    "BASE_MODEL_ID = \"./models/gemma-3-1b-it\"\n",
    "\n",
    "# 2. Fine-Tuned (LoRA) Model Path\n",
    "LORA_ADAPTER_PATH = \"./results/gemma-3-1b-it-animals-lora/checkpoint-1689\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60ea5487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model: ./models/gemma-3-1b-it\n",
      "Base Model loaded.\n",
      "Attaching LoRA Adapter from: ./results/gemma-3-1b-it-animals-lora/checkpoint-1689\n",
      "Fine-Tuned Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the base model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Loading Base Model: {BASE_MODEL_ID}\")\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=device,\n",
    ")\n",
    "base_model.eval()\n",
    "print(\"Base Model loaded.\")\n",
    "\n",
    "# Load the base model again for the fine-tuned version\n",
    "ft_tokenizer = AutoTokenizer.from_pretrained(LORA_ADAPTER_PATH) # Load the tokenizer from the checkpoint\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID, # Use the base model as the foundation\n",
    "    dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "# Load the LoRA adapter weights onto the base model\n",
    "print(f\"Attaching LoRA Adapter from: {LORA_ADAPTER_PATH}\")\n",
    "ft_model = PeftModel.from_pretrained(ft_model, LORA_ADAPTER_PATH)\n",
    "ft_model.eval()\n",
    "print(\"Fine-Tuned Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa023ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "def test_models(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    ft_prompt = ft_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    base_prompt = base_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_k\": 50,\n",
    "        \"pad_token_id\": ft_tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    # ===== USER =====\n",
    "    print(\"=\" * 50)\n",
    "    print(\"USER:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(prompt)\n",
    "\n",
    "    # ===== BASE MODEL =====\n",
    "    print(\"=\" * 50)\n",
    "    print(\"BASE MODEL RESPONSE:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    base_input_ids = base_tokenizer(base_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Create streamer for real-time output\n",
    "    base_streamer = TextStreamer(base_tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        base_output = base_model.generate(\n",
    "            **base_input_ids, \n",
    "            **generation_kwargs,\n",
    "            streamer=base_streamer\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    base_tokens_generated = len(base_output[0]) - len(base_input_ids['input_ids'][0])\n",
    "    base_duration = end_time - start_time\n",
    "    base_tokens_per_sec = base_tokens_generated / base_duration if base_duration > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTokens: {base_tokens_generated} | Time: {base_duration:.2f}s | Speed: {base_tokens_per_sec:.2f} tok/s\")\n",
    "\n",
    "    # ===== FINE-TUNED MODEL =====\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"FINE-TUNED (LoRA) MODEL RESPONSE:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    ft_input_ids = ft_tokenizer(ft_prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Create streamer for real-time output\n",
    "    ft_streamer = TextStreamer(ft_tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        ft_output = ft_model.generate(\n",
    "            **ft_input_ids, \n",
    "            **generation_kwargs,\n",
    "            streamer=ft_streamer\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ft_tokens_generated = len(ft_output[0]) - len(ft_input_ids['input_ids'][0])\n",
    "    ft_duration = end_time - start_time\n",
    "    ft_tokens_per_sec = ft_tokens_generated / ft_duration if ft_duration > 0 else 0\n",
    "    \n",
    "    print(f\"\\nTokens: {ft_tokens_generated} | Time: {ft_duration:.2f}s | Speed: {ft_tokens_per_sec:.2f} tok/s\")\n",
    "    \n",
    "    # ===== COMPARISON =====\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PERFORMANCE COMPARISON:\")\n",
    "    print(f\"Base Model:       {base_tokens_per_sec:.2f} tok/s\")\n",
    "    print(f\"Fine-Tuned Model: {ft_tokens_per_sec:.2f} tok/s\")\n",
    "    speedup = ((ft_tokens_per_sec - base_tokens_per_sec) / base_tokens_per_sec * 100) if base_tokens_per_sec > 0 else 0\n",
    "    print(f\"Difference:       {speedup:+.1f}%\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89a50969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "USER:\n",
      "--------------------------------------------------\n",
      "O que s√£o canidae?\n",
      "==================================================\n",
      "BASE MODEL RESPONSE:\n",
      "--------------------------------------------------\n",
      "Canidae √© o nome da fam√≠lia de mam√≠feros carn√≠voros que inclui c√£es, lobos, raposas, cotes, e outros. Eles s√£o animais fascinantes e complexos, com uma rica hist√≥ria evolutiva e uma variedade impressionante de caracter√≠sticas. Aqui est√° um resumo detalhado sobre as canid√©s:\n",
      "\n",
      "**1. Origem e Evolu√ß√£o:**\n",
      "\n",
      "*   **Ancestrais:** Canid√©s surgiram h√° cerca de 40 a 60 milh√µes de anos, em uma regi√£o que hoje √© a √Åsia Central.\n",
      "*   **Diversifica√ß√£o:** A fam√≠lia Canidae se diversificou em diferentes grupos ao longo de milh√µes de anos, adaptando-se a diferentes nichos ecol√≥gicos.\n",
      "*   **Origem dos Canis:** O g√™nero *Canis* √© considerado o ancestral comum de todos os canid√©s modernos.\n",
      "\n",
      "**2. Caracter√≠sticas Distintivas:**\n",
      "\n",
      "*   **Dentes:** Possuem dentes especializados para raspar carne, que se encaixam em um formato √∫nico, ideal para a mordida em carne.\n",
      "*   **Olfato:** S√£o incrivelmente sens√≠veis ao olfato, o que lhes permite detectar presas e outros animais √† dist√¢ncia. O olfato √© considerado um sentido prim√°rio na fam√≠lia Canidae.\n",
      "*   **Estrutura Social:** A maioria dos canid√©s vive em grupos sociais complexos, com hierarquias sociais bem definidas.\n",
      "*   **Comportamento:**  S√£o animais altamente inteligentes, com comportamentos complexos, incluindo comunica√ß√£o vocal, marca√ß√£o territorial e coopera√ß√£o.\n",
      "\n",
      "**3. Subgrupos Principais:**\n",
      "\n",
      "*   **C√£es (Canidae):**  A maior e mais conhecida subfam√≠lia, incluindo ra√ßas diversas com diferentes caracter√≠sticas f√≠sicas e comportamentais.\n",
      "*   **Lobos (Canini):**  Com a cabe√ßa mais larga e uma longa cauda, geralmente mais herb√≠voros.\n",
      "*   **Rapos (Canini):**  Carregam uma \"cabe√ßa de raio\" (um pequeno \"l√¢mina\" na parte de tr√°s da cabe√ßa) que usam para capturar presas.\n",
      "*   **Cotes (Canidae):**  Possuem uma pelagem densa e um focinho longo, geralmente com uma \"cabe√ßa de raio\" (um pequeno \"l√¢mina\") na parte de tr√°s.\n",
      "\n",
      "Tokens: 500 | Time: 54.21s | Speed: 9.22 tok/s\n",
      "\n",
      "==================================================\n",
      "FINE-TUNED (LoRA) MODEL RESPONSE:\n",
      "--------------------------------------------------\n",
      "As can√≠deos s√£o um grupo de mam√≠feros pertencentes √† ordem Carnivora. Eles s√£o animais que chegam a medir at√© 1,8 metros de comprimento e podem ter at√© 2,4 kg de peso. S√£o um dos grupos maiores de mam√≠feros terrestres existentes e constituem a maior parte da fauna do planeta. O grupo abrange uma grande diversidade de esp√©cies, com adapta√ß√µes diversas para diferentes ambientes. Tamb√©m √© o grupo que inclui os cachorros, c√£es, le√µes, tigres, lobos, ursos, coiotes, raposas, gatos, coiotes e, recentemente, o urso polar, que √© a √∫nica esp√©cie do grupo que vive na Ant√°rtida. √â um grupo de animais que vivem em cardumes, al√©m de serem animais sociais, e muitas esp√©cies vivem em grupos familiares ou de matilha. Obtida de \" https://pt.wikipedia.org/w/index.php?title=Canidae&oldid=46671420 \" A maioria das esp√©cies de can√≠deos s√£o predadores. S√£o animais carn√≠voros, ou seja, se alimentam principalmente de carne de outros animais. Eles possuem dentes adaptados para rasgar e triturar ossos e carne, al√©m de uma l√≠ngua comprida e afiada para agarrar as presas. Eles tamb√©m possuem uma vis√£o agu√ßada, audi√ß√£o e olfato muito desenvolvidos. Apesar de serem animais carn√≠voros, algumas esp√©cies de can√≠deos s√£o on√≠voras ou herb√≠voras, sendo o urso pardo o exemplo mais not√≥rio. Eles tamb√©m s√£o conhecidos pelos seus comportamentos complexos, como a comunica√ß√£o atrav√©s de latidos, uivos, linguagem corporal e at√© mesmo a cria√ß√£o de sociedades. As rela√ß√µes entre as diferentes esp√©cies de can√≠deos ainda s√£o objeto de estudo e existem muitas esp√©cies ainda a serem descobertas. H√° tamb√©m a quest√£o do parentesco dos felinos, com muitos estudos indicando que eles descendem de um √∫nico ancestral comum que viveu no final do Jur√°ssico. Obtida de \" https://pt.wikipedia.org/w/index.php?title=Canidae&oldid=46671420 \" Classifica√ß√£o do grupo Os can√≠deos podem ser divididos em dois grupos principais: os can√≠deos primitivos e os can√≠deos modernos. Os can√≠deos\n",
      "\n",
      "Tokens: 500 | Time: 148.70s | Speed: 3.36 tok/s\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE COMPARISON:\n",
      "Base Model:       9.22 tok/s\n",
      "Fine-Tuned Model: 3.36 tok/s\n",
      "Difference:       -63.5%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Define your test prompt\n",
    "# prompt = \"A baleia azul √©\"\n",
    "# prompt = \"Fale sobre o le√£o\"\n",
    "prompt = \"O que s√£o canidae?\"\n",
    "\n",
    "test_models(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb05bc",
   "metadata": {},
   "source": [
    "---\n",
    "USER:\n",
    "---\n",
    "O que s√£o canidae?\n",
    "\n",
    "---\n",
    "BASE MODEL RESPONSE:\n",
    "\n",
    "---\n",
    "Canidae √© o nome da fam√≠lia de mam√≠feros carn√≠voros que inclui c√£es, lobos, raposas, cotes, e outros. Eles s√£o animais fascinantes e complexos, com uma rica hist√≥ria evolutiva e uma variedade impressionante de caracter√≠sticas. Aqui est√° um resumo detalhado sobre as canid√©s:\n",
    "\n",
    "**1. Origem e Evolu√ß√£o:**\n",
    "\n",
    "*   **Ancestrais:** Canid√©s surgiram h√° cerca de 40 a 60 milh√µes de anos, em uma regi√£o que hoje √© a √Åsia Central.\n",
    "*   **Diversifica√ß√£o:** A fam√≠lia Canidae se diversificou em diferentes grupos ao longo de milh√µes de anos, adaptando-se a diferentes nichos ecol√≥gicos.\n",
    "*   **Origem dos Canis:** O g√™nero *Canis* √© considerado o ancestral comum de todos os canid√©s modernos.\n",
    "\n",
    "**2. Caracter√≠sticas Distintivas:**\n",
    "\n",
    "*   **Dentes:** Possuem dentes especializados para raspar carne, que se encaixam em um formato √∫nico, ideal para a mordida em carne.\n",
    "*   **Olfato:** S√£o incrivelmente sens√≠veis ao olfato, o que lhes permite detectar presas e outros animais √† dist√¢ncia. O olfato √© considerado um sentido prim√°rio na fam√≠lia Canidae.\n",
    "*   **Estrutura Social:** A maioria dos canid√©s vive em grupos sociais complexos, com hierarquias sociais bem definidas.\n",
    "*   **Comportamento:**  S√£o animais altamente inteligentes, com comportamentos complexos, incluindo comunica√ß√£o vocal, marca√ß√£o territorial e coopera√ß√£o.\n",
    "\n",
    "**3. Subgrupos Principais:**\n",
    "\n",
    "*   **C√£es (Canidae):**  A maior e mais conhecida subfam√≠lia, incluindo ra√ßas diversas com diferentes caracter√≠sticas f√≠sicas e comportamentais.\n",
    "*   **Lobos (Canini):**  Com a cabe√ßa mais larga e uma longa cauda, geralmente mais herb√≠voros.\n",
    "*   **Rapos (Canini):**  Carregam uma \"cabe√ßa de raio\" (um pequeno \"l√¢mina\" na parte de tr√°s da cabe√ßa) que usam para capturar presas.\n",
    "*   **Cotes (Canidae):**  Possuem uma pelagem densa e um focinho longo, geralmente com uma \"cabe√ßa de raio\" (um pequeno \"l√¢mina\") na parte de tr√°s.\n",
    "\n",
    "Tokens: 500 | Time: 54.21s | Speed: 9.22 tok/s\n",
    "\n",
    "---\n",
    "FINE-TUNED (LoRA) MODEL RESPONSE:\n",
    "\n",
    "---\n",
    "As can√≠deos s√£o um grupo de mam√≠feros pertencentes √† ordem Carnivora. Eles s√£o animais que chegam a medir at√© 1,8 metros de comprimento e podem ter at√© 2,4 kg de peso. S√£o um dos grupos maiores de mam√≠feros terrestres existentes e constituem a maior parte da fauna do planeta. O grupo abrange uma grande diversidade de esp√©cies, com adapta√ß√µes diversas para diferentes ambientes. Tamb√©m √© o grupo que inclui os cachorros, c√£es, le√µes, tigres, lobos, ursos, coiotes, raposas, gatos, coiotes e, recentemente, o urso polar, que √© a √∫nica esp√©cie do grupo que vive na Ant√°rtida. √â um grupo de animais que vivem em cardumes, al√©m de serem animais sociais, e muitas esp√©cies vivem em grupos familiares ou de matilha. Obtida de \" https://pt.wikipedia.org/w/index.php?title=Canidae&oldid=46671420 \" A maioria das esp√©cies de can√≠deos s√£o predadores. S√£o animais carn√≠voros, ou seja, se alimentam principalmente de carne de outros animais. Eles possuem dentes adaptados para rasgar e triturar ossos e carne, al√©m de uma l√≠ngua comprida e afiada para agarrar as presas. Eles tamb√©m possuem uma vis√£o agu√ßada, audi√ß√£o e olfato muito desenvolvidos. Apesar de serem animais carn√≠voros, algumas esp√©cies de can√≠deos s√£o on√≠voras ou herb√≠voras, sendo o urso pardo o exemplo mais not√≥rio. Eles tamb√©m s√£o conhecidos pelos seus comportamentos complexos, como a comunica√ß√£o atrav√©s de latidos, uivos, linguagem corporal e at√© mesmo a cria√ß√£o de sociedades. As rela√ß√µes entre as diferentes esp√©cies de can√≠deos ainda s√£o objeto de estudo e existem muitas esp√©cies ainda a serem descobertas. H√° tamb√©m a quest√£o do parentesco dos felinos, com muitos estudos indicando que eles descendem de um √∫nico ancestral comum que viveu no final do Jur√°ssico. Obtida de \" https://pt.wikipedia.org/w/index.php?title=Canidae&oldid=46671420 \" Classifica√ß√£o do grupo Os can√≠deos podem ser divididos em dois grupos principais: os can√≠deos primitivos e os can√≠deos modernos. Os can√≠deos\n",
    "\n",
    "Tokens: 500 | Time: 148.70s | Speed: 3.36 tok/s\n",
    "\n",
    "---\n",
    "PERFORMANCE COMPARISON:\n",
    "- Base Model:       9.22 tok/s\n",
    "- Fine-Tuned Model: 3.36 tok/s\n",
    "- Difference:       -63.5%\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f50c40",
   "metadata": {},
   "source": [
    "## Notebook topics ‚Äî demo.ipynb\n",
    "\n",
    "- **Environment & libraries**\n",
    "  - Listed and imported core Python libraries used across the notebook: numpy, pandas, psutil, torch, BeautifulSoup, datasets, transformers, peft, etc.\n",
    "  - Performed a quick CUDA/PyTorch availability check and printed device/CUDA info.\n",
    "\n",
    "- **Data import**\n",
    "  - Read media_resource.tab (TSV) into a DataFrame.\n",
    "  - Printed shape and columns to inspect the dataset.\n",
    "\n",
    "- **Data cleaning**\n",
    "  - Implemented `clean_html_wikipedia(text)` to:\n",
    "    - Strip HTML using BeautifulSoup.\n",
    "    - Remove \"Refer√™ncias\" and typical Wikipedia artifacts (edit links, bracketed references).\n",
    "    - Normalize whitespace and punctuation.\n",
    "  - Applied the cleaning function to create a `text` column.\n",
    "\n",
    "- **Deduplication & column selection**\n",
    "  - Dropped duplicate rows by `identifier`.\n",
    "  - Selected and renamed relevant fields to a cleaned DataFrame (`id`, `title`, `text`, `url`).\n",
    "\n",
    "- **Dataset preparation & saving**\n",
    "  - Converted the cleaned DataFrame into a Hugging Face `Dataset`.\n",
    "  - Saved the dataset to animal_wikipedia_pt_dataset (disk cache / arrow files present in the repo).\n",
    "\n",
    "- **Model selection**\n",
    "  - Set base model path: gemma-3-1b-it (local model files present).\n",
    "\n",
    "- **Train / Eval split**\n",
    "  - Loaded dataset from disk and created a train/test split (90/10).\n",
    "  - Sampled up to 9k training examples and up to 1k evaluation examples.\n",
    "\n",
    "- **Tokenization**\n",
    "  - Loaded tokenizer from the base model and set `pad_token` to `eos_token`.\n",
    "  - Tokenization function appends EOS, truncates to 512 tokens, and returns attention masks.\n",
    "  - Tokenized train and eval splits and computed total / average token counts for each.\n",
    "\n",
    "- **Data collator**\n",
    "  - Created a `DataCollatorForLanguageModeling` for causal LM (mlm=False) with padding optimizations.\n",
    "\n",
    "- **Model loading & quantization**\n",
    "  - Defined a 4-bit quantization configuration (`BitsAndBytesConfig` with nf4, double quant).\n",
    "  - Loaded the model with 4-bit weights using device auto-mapping.\n",
    "  - Enabled gradient checkpointing and input-requires-grad for k-bit training.\n",
    "\n",
    "- **LoRA setup**\n",
    "  - Configured LoRA with `r=16`, target projection modules, alpha, dropout, and CAUSAL_LM task.\n",
    "  - Applied LoRA (`get_peft_model`) and printed trainable parameter stats.\n",
    "\n",
    "- **Training configuration**\n",
    "  - Prepared `TrainingArguments` for LoRA training:\n",
    "    - 3 epochs, effective batch size via gradient accumulation, learning rate, warmup, cosine scheduler.\n",
    "    - Evaluation every 250 steps, checkpointing, logging, mixed precision (fp16), optimizer (paged_adamw_8bit), gradient checkpointing.\n",
    "    - Output dir pattern: `./results/{model_name}-animals-lora`.\n",
    "  - Created a `Trainer` and invoked `trainer.train()` (training cell present).\n",
    "\n",
    "- **System & memory diagnostics**\n",
    "  - Added helper functions to report GPU (via `nvidia-smi`) and system RAM usage (psutil).\n",
    "  - Printed detailed PyTorch CUDA memory stats (allocated, cached, max) and a short forward/generation sanity test to confirm model behavior.\n",
    "\n",
    "- **Saving / checkpoints**\n",
    "  - Notebook uses Hugging Face Trainer defaults plus `save_steps` and `save_total_limit`; results are stored under results (several checkpoints and adapter files visible in the repo).\n",
    "\n",
    "- **Loading & testing fine-tuned LoRA adapter**\n",
    "  - Demonstrated loading the base model and reloading it to attach a LoRA adapter from a checkpoint (e.g., checkpoint-1689).\n",
    "  - Showed how to apply the model-specific chat template and generate text from both base and fine-tuned models, printing only the assistant response.\n",
    "  - Generation parameters used: sampling, temperature, top-k, and pad/eos handling.\n",
    "\n",
    "- **Notable artifacts & files**\n",
    "  - Cleaned dataset saved at animal_wikipedia_pt_dataset (arrow cache files present).\n",
    "  - Model files in gemma-3-1b-it and gemma-2b.\n",
    "  - Training outputs and LoRA checkpoints under results (including adapter_model.safetensors, tokenizer files, trainer_state, and checkpoint meta like `chat_template.jinja`).\n",
    "\n",
    "- **Purpose summary (one-line)**\n",
    "  - Prepare a Portuguese Wikipedia-derived animal text dataset, fine-tune a Gemma causal LM with LoRA on the dataset using 4-bit quantization and memory-efficient training, and validate generation from base vs. fine-tuned adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1fb74",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc-2s2025-ssrl-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
